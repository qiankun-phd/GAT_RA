# æœ€å°åŒ–ä¿®æ”¹æ–¹æ¡ˆ

**åŸºäº**: `/home/qiankun/GAT_RA/origin/` åŸå§‹ä»£ç   
**ç›®æ ‡**: æ·»åŠ è¯­ä¹‰é€šä¿¡ï¼ˆå‹ç¼©æ¯”rhoï¼‰å’ŒGATå¼€å…³  
**åŸåˆ™**: ä¿æŒåŸå§‹è®­ç»ƒæµç¨‹ï¼Œåšæœ€å°æ”¹åŠ¨

---

## ğŸ“‹ åŸå§‹ä»£ç ç»“æ„

### main_PPO_AC.py
```python
# åŠ¨ä½œç»´åº¦
action_dim = 2  # RB_choice + power

# è®­ç»ƒæ•°æ®
action_all_training = np.zeros([n_veh, 2])  # [RB, power]

# æ‰§è¡ŒåŠ¨ä½œ
train_reward = env.act_for_training(action_temp, IS_PPO)

# è®­ç»ƒ
loss = ppoes.train(s, a, gae, reward, v_pred_next, sess)
```

### PPO_brain_AC.py
```python
# ç½‘ç»œè¾“å‡º
mu, sigma = ...  # Power (Normalåˆ†å¸ƒ)
RB_probs = ...   # RB (Categoricalåˆ†å¸ƒ)

# åŠ¨ä½œé‡‡æ ·
choose_action_op = concat([RB_sample, power_sample])  # [RB, power]
```

---

## âœ… æœ€å°åŒ–ä¿®æ”¹æ–¹æ¡ˆ

### 1. æ·»åŠ å‹ç¼©æ¯”è¾“å‡ºï¼ˆPPO_brain_AC.pyï¼‰

#### ç½‘ç»œå®šä¹‰
```python
# åœ¨ _build_net ä¸­æ·»åŠ 
self.w_rho_alpha = tf.Variable(initializer(shape=(n_hidden_2, 1)), trainable=trainable)
self.w_rho_beta = tf.Variable(initializer(shape=(n_hidden_2, 1)), trainable=trainable)
self.b_rho_alpha = tf.Variable(tf.truncated_normal([1], stddev=0.1), trainable=trainable)
self.b_rho_beta = tf.Variable(tf.truncated_normal([1], stddev=0.1), trainable=trainable)

# æ·»åŠ Betaåˆ†å¸ƒï¼ˆç”¨äºå‹ç¼©æ¯” rho âˆˆ [0,1]ï¼‰
rho_alpha = tf.nn.softplus(tf.add(tf.matmul(layer_2_b, self.w_rho_alpha), self.b_rho_alpha)) + 1.0
rho_beta = tf.nn.softplus(tf.add(tf.matmul(layer_2_b, self.w_rho_beta), self.b_rho_beta)) + 1.0
rho_distribution = tf.distributions.Beta(rho_alpha, rho_beta)

# è¿”å›
return norm_dist, RB_distribution, rho_distribution, v, params, saver
```

#### åŠ¨ä½œé‡‡æ ·
```python
# ä¿®æ”¹ choose_action_op
self.choose_action_op = tf.concat([
    tf.transpose(tf.cast(RB_distribution.sample(1), dtype=tf.float32)),
    tf.squeeze(pi.sample(1), axis=0),
    tf.squeeze(rho_distribution.sample(1), axis=0)  # æ·»åŠ rho
], 1)
```

#### Losså‡½æ•°
```python
# åœ¨åŸæœ‰åŸºç¡€ä¸Šæ·»åŠ rhoçš„loss
rho_action = self.a[:,2]
ratio_rho = rho_distribution.prob(rho_action) / old_rho_distribution.prob(rho_action)
L_rho = tf.reduce_mean(tf.minimum(
    ratio_rho * GAE_advantage,
    tf.clip_by_value(ratio_rho, 1 - epsilon, 1 + epsilon) * GAE_advantage
))

# æ›´æ–°æ€»Loss
L = L_clip + L_RB + L_rho - c1 * L_vf + c2 * S
```

### 2. æ·»åŠ GATå¼€å…³

#### PPO_brain_AC.py
```python
def __init__(self, s_dim, a_bound, c1, c2, epsilon, lr, meta_lr, K, n_veh, n_RB, 
             IS_meta, meta_episode, use_gat=False):
    self.use_gat = use_gat
    # ...
    
    if use_gat:
        # GATæ¨¡å¼
        pi, RB_dist, rho_dist, self.v, params, self.saver = self._build_net_gat(...)
    else:
        # MLPæ¨¡å¼ï¼ˆåŸå§‹ï¼‰
        pi, RB_dist, rho_dist, self.v, params, self.saver = self._build_net(...)
```

### 3. æ›´æ–°main_PPO_AC.py

```python
# åŠ¨ä½œç»´åº¦
action_dim = 3  # RB_choice + power + rho

# åŠ¨ä½œè¾¹ç•Œ
action_bound = [n_RB, args.RB_action_bound, 1.0]  # æ·»åŠ rhoè¾¹ç•Œ

# è®­ç»ƒæ•°æ®
action_all_training = np.zeros([n_veh, 3])  # [RB, power, rho]

# PPOåˆå§‹åŒ–
use_gat = args.use_gat if hasattr(args, 'use_gat') else False
ppoes = PPO(state_dim, action_bound, ..., use_gat=use_gat)
```

---

## ğŸ”§ å…·ä½“ä¿®æ”¹æ­¥éª¤

### Step 1: ä¿®æ”¹PPO_brain_AC.py

1. æ·»åŠ rhoç›¸å…³çš„ç½‘ç»œå‚æ•°ï¼ˆ6è¡Œï¼‰
2. ä¿®æ”¹`_build_net`è¿”å›å€¼ï¼ˆ1è¡Œï¼‰
3. ä¿®æ”¹`choose_action_op`ï¼ˆ1è¡Œï¼‰
4. ä¿®æ”¹Losså‡½æ•°ï¼ˆ5è¡Œï¼‰
5. ä¿®æ”¹`choose_action`çš„è£å‰ªï¼ˆ1è¡Œï¼‰
6. ä¿®æ”¹`averaging_model`ï¼ˆ2è¡Œï¼‰

**æ€»å…±çº¦15è¡Œä»£ç ä¿®æ”¹**

### Step 2: ä¿®æ”¹main_PPO_AC.py

1. ä¿®æ”¹action_dimï¼ˆ1è¡Œï¼‰
2. ä¿®æ”¹action_boundï¼ˆ1è¡Œï¼‰
3. ä¿®æ”¹action_all_trainingï¼ˆ1è¡Œï¼‰
4. æ·»åŠ use_gatå‚æ•°ï¼ˆ1è¡Œï¼‰

**æ€»å…±çº¦4è¡Œä»£ç ä¿®æ”¹**

### Step 3: æ·»åŠ GATå¼€å…³ï¼ˆå¯é€‰ï¼‰

1. åœ¨arguments.pyæ·»åŠ å‚æ•°ï¼ˆ3è¡Œï¼‰
2. åœ¨PPO_brain_AC.pyæ·»åŠ GATç½‘ç»œï¼ˆå¤ç”¨ç°æœ‰ä»£ç ï¼‰

---

## ğŸ“Š ä¿®æ”¹å¯¹æ¯”

| æ–‡ä»¶ | åŸå§‹è¡Œæ•° | æ–°å¢è¡Œæ•° | ä¿®æ”¹è¡Œæ•° | æ€»è¡Œæ•° |
|------|---------|---------|---------|--------|
| PPO_brain_AC.py | 301 | ~15 | ~5 | ~320 |
| main_PPO_AC.py | 301 | ~4 | ~4 | ~305 |
| arguments.py | - | ~3 | - | - |
| **æ€»è®¡** | 602 | **~22** | **~9** | **~625** |

**ä»£ç å¢åŠ **: ä¸åˆ°4%

---

## ğŸ¯ ä¿æŒä¸å˜çš„éƒ¨åˆ†

1. âœ… è®­ç»ƒæµç¨‹ï¼ˆsimulate â†’ sample â†’ trainï¼‰
2. âœ… GAEè®¡ç®—
3. âœ… å¥–åŠ±å½’ä¸€åŒ–
4. âœ… è”é‚¦å­¦ä¹ é€»è¾‘
5. âœ… TensorBoardæ—¥å¿—
6. âœ… æ¨¡å‹ä¿å­˜/åŠ è½½

---

## ğŸ”„ ä¿®æ”¹åçš„æ•°æ®æµ

### åŸå§‹æµç¨‹
```
State â†’ PPO â†’ [RB, Power] â†’ Environment â†’ Reward
```

### ä¿®æ”¹åæµç¨‹
```
State â†’ PPO â†’ [RB, Power, Rho] â†’ Environment (Semantic) â†’ Reward
```

**å˜åŒ–**: åªå¢åŠ ä¸€ä¸ªè¾“å‡ºç»´åº¦

---

## ğŸ“ å®æ–½é¡ºåº

1. **å…ˆä¸åŠ GAT**: åªæ·»åŠ rhoè¾“å‡ºï¼Œä½¿ç”¨åŸå§‹çš„MLPç½‘ç»œ
2. **éªŒè¯è®­ç»ƒ**: ç¡®ä¿è®­ç»ƒæµç¨‹æ­£å¸¸
3. **åç»­æ·»åŠ GAT**: ä½œä¸ºå¯é€‰å¼€å…³

è¿™æ ·å¯ä»¥é€æ­¥éªŒè¯ï¼Œé™ä½é£é™©ã€‚

