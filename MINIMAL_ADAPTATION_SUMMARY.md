# æœ€å°åŒ–è¯­ä¹‰é€šä¿¡é€‚é…æ€»ç»“

**æ—¥æœŸ**: 2025-12-10  
**åŸåˆ™**: åœ¨åŸå§‹ä»£ç ç»“æ„ä¸Šåšæœ€å°æ”¹åŠ¨

---

## ğŸ¯ é€‚é…ç­–ç•¥

### åŸå§‹ä»£ç ç»“æ„ï¼ˆä¿æŒä¸å˜ï¼‰
```python
# åŸå§‹ act_for_training
for i in range(len(self.success)):
    if (self.success[i] == 1) and (cellular_SINR[i] > training_sinr_threshold):
        SE_sum += SE[i]
        EE_sum += EE[i]
    else:
        # å¤±è´¥æƒ©ç½š
        SE_sum = (np.sum(self.success) - self.n_Veh) / self.n_Veh
        EE_sum = (np.sum(self.success) - self.n_Veh) / self.n_Veh
        break

reward = (self.beta * SE_sum + (1 - self.beta) * EE_sum) / self.n_Veh
```

### é€‚é…åï¼ˆæœ€å°ä¿®æ”¹ï¼‰
```python
# é€‚é…è¯­ä¹‰é€šä¿¡
for i in range(len(self.success)):
    if (self.success[i] == 1) and (cellular_SINR[i] > training_sinr_threshold):
        SE_sum += SE[i]
        Semantic_EE_sum += semantic_EE[i]  # ç”¨Semantic-EEæ›¿æ¢EE
    else:
        # å¤±è´¥æƒ©ç½šï¼ˆä¿æŒä¸å˜ï¼‰
        SE_sum = (np.sum(self.success) - self.n_Veh) / self.n_Veh
        Semantic_EE_sum = (np.sum(self.success) - self.n_Veh) / self.n_Veh
        break

reward = (self.beta * SE_sum + (1 - self.beta) * Semantic_EE_sum) / self.n_Veh
```

---

## âœ… ä¿®æ”¹å†…å®¹

### 1. Environment_marl_indoor.py

#### åˆ é™¤çš„å‚æ•°ï¼ˆ__init__ï¼‰
```python
# åˆ é™¤
collision_penalty=-0.5
low_accuracy_penalty=-0.3  
accuracy_threshold=0.5
```

#### ç®€åŒ–çš„ act_for_training
```python
def act_for_training(self, actions, IS_PPO):
    """æœ€å°åŒ–é€‚é…ï¼šåªç”¨Semantic-EEæ›¿æ¢EE"""
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    results = self.Compute_Performance_Reward_Train(action_temp, is_ppo_mode)
    (cellular_Rate, cellular_SINR, SE, EE, 
     semantic_accuracy, semantic_EE, collisions) = results
    
    # ä¸åŸå§‹ä»£ç ç›¸åŒçš„ç»“æ„
    SE_sum = 0.0
    Semantic_EE_sum = 0.0
    training_sinr_threshold = 3.3
    
    for i in range(len(self.success)):
        if (self.success[i] == 1) and (cellular_SINR[i] > training_sinr_threshold):
            SE_sum += SE[i]
            Semantic_EE_sum += semantic_EE[i]  # å”¯ä¸€çš„å…³é”®ä¿®æ”¹
        else:
            SE_sum = (np.sum(self.success) - self.n_Veh) / self.n_Veh
            Semantic_EE_sum = (np.sum(self.success) - self.n_Veh) / self.n_Veh
            break
    
    # ä¸åŸå§‹ä»£ç ç›¸åŒçš„å¥–åŠ±è®¡ç®—
    if self.optimization_target == 'SE':
        reward = SE_sum / self.n_Veh
    elif self.optimization_target == 'EE':
        reward = Semantic_EE_sum / self.n_Veh
    elif self.optimization_target == 'SE_EE':
        reward = (self.beta * SE_sum + (1 - self.beta) * Semantic_EE_sum) / self.n_Veh
    else:
        reward = (self.beta * SE_sum + (1 - self.beta) * Semantic_EE_sum) / self.n_Veh
    
    return reward
```

#### åˆ é™¤çš„penaltiesé€»è¾‘
```python
# åˆ é™¤
penalties = np.zeros(self.n_Veh)
for i in range(len(self.vehicles)):
    if collisions[i] > 0:
        penalties[i] += self.collision_penalty
    if semantic_accuracy[i] < self.accuracy_threshold:
        penalties[i] += self.low_accuracy_penalty

# æ”¹ä¸º
semantic_EE_penalized = semantic_EE  # ä¸æ·»åŠ é¢å¤–æƒ©ç½š
```

### 2. arguments.py

#### åˆ é™¤çš„å‚æ•°
```python
# åˆ é™¤ï¼ˆæˆ–æ³¨é‡Šï¼‰
# parser.add_argument('--collision_penalty', ...)
# parser.add_argument('--low_accuracy_penalty', ...)
# parser.add_argument('--accuracy_threshold', ...)
```

### 3. main_PPO_AC.py

#### ç¯å¢ƒåˆå§‹åŒ–ï¼ˆéœ€è¦æ›´æ–°ï¼‰
```python
# åŸæ¥
env = Environ(n_veh, n_RB, ..., 
              collision_penalty=args.collision_penalty,
              low_accuracy_penalty=args.low_accuracy_penalty,
              accuracy_threshold=args.accuracy_threshold)

# æ”¹ä¸º
env = Environ(n_veh, n_RB, ...,
              semantic_A_max=args.semantic_A_max,
              semantic_beta=args.semantic_beta)
```

---

## ğŸ”„ å¯¹æ¯”ï¼šå¤æ‚ç‰ˆæœ¬ vs ç®€å•ç‰ˆæœ¬

### å¤æ‚ç‰ˆæœ¬ï¼ˆä¹‹å‰ï¼‰
- âŒ SEMANTIC_REWARD_SCALEæ”¾å¤§
- âŒ åŠŸç‡æƒ©ç½š
- âŒ ä½å‡†ç¡®åº¦æƒ©ç½š
- âŒ ç¢°æ’æƒ©ç½šå•ç‹¬å¤„ç†
- âŒ reward_componentså­—å…¸
- âŒ å¤æ‚çš„è¯Šæ–­é€»è¾‘

### ç®€å•ç‰ˆæœ¬ï¼ˆç°åœ¨ï¼‰
- âœ… åªç”¨Semantic-EEæ›¿æ¢EE
- âœ… ä¿æŒåŸå§‹å¥–åŠ±ç»“æ„
- âœ… å¤±è´¥æƒ©ç½šé€šè¿‡successæ ‡å¿—å¤„ç†
- âœ… ç®€å•æ¸…æ™°ï¼Œæ˜“äºç†è§£

---

## ğŸ“Š ä¸ºä»€ä¹ˆè¿™æ ·æ›´å¥½ï¼Ÿ

### 1. Semantic-EEå·²åŒ…å«æ‰€æœ‰è€ƒè™‘

```
Semantic-EE = Semantic_Accuracy / (P_tx + P_circuit)
               â†‘ä¼˜åŒ–å‡†ç¡®åº¦        â†‘ä¼˜åŒ–èƒ½æ•ˆ
```

- **åˆ†å­**ï¼šè¯­ä¹‰å‡†ç¡®åº¦ï¼ˆé€šä¿¡è´¨é‡ï¼‰
- **åˆ†æ¯**ï¼šåŠŸç‡æ¶ˆè€—ï¼ˆèƒ½é‡æ•ˆç‡ï¼‰
- ä¸éœ€è¦é¢å¤–çš„æƒ©ç½šé¡¹

### 2. åŸå§‹å¤±è´¥æœºåˆ¶å·²ç»è¶³å¤Ÿ

```python
if (self.success[i] == 1) and (cellular_SINR[i] > threshold):
    # æˆåŠŸï¼šæ­£å¥–åŠ±
else:
    # å¤±è´¥ï¼šè´Ÿæƒ©ç½š
    break
```

- ç¢°æ’ â†’ success[i] = 0 â†’ å¤±è´¥
- SINRä¸è¶³ â†’ å¤±è´¥
- ä¸éœ€è¦é¢å¤–çš„collision_penalty

### 3. ç®€å•å°±æ˜¯ç¾

- ä»£ç å°‘ â†’ bugå°‘
- é€»è¾‘æ¸…æ™° â†’ æ˜“äºç†è§£
- å‚æ•°å°‘ â†’ æ˜“äºè°ƒä¼˜
- ç¬¦åˆåŸå§‹è®¾è®¡ç†å¿µ

---

## âš ï¸ éœ€è¦æ›´æ–°çš„æ–‡ä»¶

### å¿…é¡»ä¿®æ”¹
1. âœ… Environment_marl_indoor.py
   - __init__: åˆ é™¤3ä¸ªå‚æ•°
   - act_for_training: ç®€åŒ–ä¸ºåŸå§‹ç»“æ„
   - Compute_Performance_Reward_Train: åˆ é™¤penalties

2. â³ main_PPO_AC.py
   - ç¯å¢ƒåˆå§‹åŒ–ï¼šåˆ é™¤ä¼ é€’çš„å‚æ•°

3. â³ arguments.py  
   - åˆ é™¤æˆ–æ³¨é‡Šç›¸å…³å‚æ•°

### å¯é€‰ä¿®æ”¹
- simulate()å‡½æ•°ï¼šå¯èƒ½ä¸å†éœ€è¦è¿”å›reward_components
- TensorBoardæ—¥å¿—ï¼šç®€åŒ–ä¸ºåªè®°å½•æ€»å¥–åŠ±

---

## ğŸ¯ é¢„æœŸæ•ˆæœ

### å¥–åŠ±ç»„ä»¶
```
reward = beta * SE + (1-beta) * Semantic-EE
```

- å½“ beta=0.5:
  - SEå 50%
  - Semantic-EEå 50%
  - ç®€å•å¹³è¡¡

### å¤±è´¥å¤„ç†
```
success=0 æˆ– SINR<3.3 â†’ è´Ÿæƒ©ç½š
```
- ä¸åŸå§‹ä»£ç å®Œå…¨ä¸€è‡´
- ç®€å•æœ‰æ•ˆ

---

## ğŸ“ TODO

- [ ] æ›´æ–°main_PPO_AC.pyä¸­çš„ç¯å¢ƒåˆå§‹åŒ–
- [ ] åˆ é™¤arguments.pyä¸­çš„ç›¸å…³å‚æ•°
- [ ] ç®€åŒ–simulate()å‡½æ•°ï¼ˆå¯é€‰ï¼‰
- [ ] ç®€åŒ–TensorBoardæ—¥å¿—ï¼ˆå¯é€‰ï¼‰
- [ ] é‡æ–°è®­ç»ƒæµ‹è¯•

---

**æ€»ç»“**: 
å›åˆ°åŸå§‹ç®€æ´çš„è®¾è®¡ï¼Œåªåšæœ€å°çš„è¯­ä¹‰é€šä¿¡é€‚é…ã€‚
ç”¨Semantic-EEæ›¿æ¢EEï¼Œä¿æŒå…¶ä»–é€»è¾‘ä¸å˜ã€‚

