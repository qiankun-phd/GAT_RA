# å¥–åŠ±ç¼©æ”¾é—®é¢˜è¯Šæ–­æŠ¥å‘Š

**åˆ†ææ—¥æœŸ**: 2025-12-10  
**é—®é¢˜**: å¥–åŠ±å‡½æ•°ç¼©æ”¾ä¸¥é‡å¤±è¡¡ï¼Œå¯¼è‡´è®­ç»ƒä¸æ”¶æ•›

---

## ğŸ”´ æ ¸å¿ƒé—®é¢˜

### å¥–åŠ±ç»„ä»¶å æ¯”ä¸¥é‡å¤±è¡¡

| ç»„ä»¶ | ç»å¯¹å€¼å‡å€¼ | å æ¯” | çŠ¶æ€ |
|------|-----------|------|------|
| **è¯­ä¹‰å‡†ç¡®åº¦å¥–åŠ±** | 0.0401 | **8.75%** | ğŸ”´ **è¢«ä¸¥é‡å‹åˆ¶** |
| åŠŸç‡æƒ©ç½š | 0.0293 | 6.40% | âœ“ æ­£å¸¸ |
| **ç¢°æ’æƒ©ç½š** | 0.1625 | **35.50%** | âš ï¸ è¿‡å¤§ |
| **ä½å‡†ç¡®åº¦æƒ©ç½š** | 0.2259 | **49.35%** | ğŸ”´ **ä¸»å¯¼å¥–åŠ±** |

### å…³é”®å‘ç°

1. **è¯­ä¹‰å¥–åŠ±è¢«å‹åˆ¶**
   - è¯­ä¹‰å‡†ç¡®åº¦å¥–åŠ±åªå æ€»å¥–åŠ±çš„8.75%
   - æƒ©ç½šé¡¹æ€»å’Œæ˜¯è¯­ä¹‰å¥–åŠ±çš„10.42å€
   - **æ¯”ä¾‹**: `1:10.42` (è¯­ä¹‰å¥–åŠ±:æ€»æƒ©ç½š)

2. **ä½å‡†ç¡®åº¦æƒ©ç½šä¸»å¯¼**
   - ä½å‡†ç¡®åº¦æƒ©ç½šå 49.35%ï¼Œæ˜¯æœ€å¤§çš„å•ä¸€ç»„ä»¶
   - å‡å€¼-0.226ï¼Œè¿œå¤§äºè¯­ä¹‰å¥–åŠ±0.040

3. **ç¢°æ’æƒ©ç½šè¿‡å¤§**
   - å 35.50%ï¼Œæ˜¯ç¬¬äºŒå¤§ç»„ä»¶
   - å‡å€¼-0.162ï¼Œæ˜¯è¯­ä¹‰å¥–åŠ±çš„4å€

---

## ğŸ” é—®é¢˜æ ¹æº

### å½“å‰å¥–åŠ±å‡½æ•°å®ç°

```python
# ç¯å¢ƒä»£ç  (Environment_marl_indoor.py)
def act_for_training(self, actions, IS_PPO):
    semantic_accuracy_reward = 0.0
    power_penalty = 0.0
    collision_penalty = 0.0
    low_accuracy_penalty = 0.0
    
    for i in range(len(self.success)):
        if self.success[i] == 1:
            # è¯­ä¹‰å‡†ç¡®åº¦å¥–åŠ±
            base_semantic_EE = semantic_accuracy[i] / total_power
            semantic_accuracy_reward += base_semantic_EE  # å€¼å¾ˆå°ï¼Œçº¦0.04
            
            # åŠŸç‡æƒ©ç½š
            power_penalty -= transmission_power_linear[i] * 0.001  # çº¦-0.03
        else:
            # å¤±è´¥æƒ©ç½š
            if collisions[i] > 0:
                collision_penalty += self.collision_penalty  # -0.5 æ¯æ¬¡
            if semantic_accuracy[i] < self.accuracy_threshold:
                low_accuracy_penalty += self.low_accuracy_penalty  # -0.3 æ¯æ¬¡
    
    # å½’ä¸€åŒ–
    if successful_count > 0:
        reward = semantic_EE_sum / self.n_Veh  # é™¤ä»¥6
        semantic_accuracy_reward = semantic_accuracy_reward / self.n_Veh
    
    # å½’ä¸€åŒ–æƒ©ç½š
    collision_penalty = collision_penalty / self.n_Veh
    low_accuracy_penalty = low_accuracy_penalty / self.n_Veh
    power_penalty = power_penalty / self.n_Veh
```

### é—®é¢˜åˆ†æ

1. **è¯­ä¹‰EEæœ¬èº«å°±å¾ˆå°**
   - è¯­ä¹‰å‡†ç¡®åº¦: çº¦0.3-0.8
   - åŠŸç‡: çº¦0.5-2.0 W
   - `EE = accuracy / power â‰ˆ 0.3-1.5`
   - å½’ä¸€åŒ–å: `EE/6 â‰ˆ 0.05-0.25`

2. **å›ºå®šæƒ©ç½šå€¼è¿‡å¤§**
   - `collision_penalty = -0.5` (åˆå§‹åŒ–å‚æ•°)
   - `low_accuracy_penalty = -0.3` (åˆå§‹åŒ–å‚æ•°)
   - è¿™äº›å€¼è¿œå¤§äºè¯­ä¹‰EE

3. **æƒ©ç½šç´¯ç§¯æ•ˆåº”**
   - å¦‚æœ4ä¸ªUEå¤±è´¥ï¼Œä½å‡†ç¡®åº¦æƒ©ç½š = -0.3 * 4 / 6 = -0.2
   - å¦‚æœè¿™4ä¸ªUEè¿˜æœ‰ç¢°æ’ï¼Œç¢°æ’æƒ©ç½š = -0.5 * 4 / 6 = -0.33
   - æ€»æƒ©ç½š: -0.53ï¼Œè¿œå¤§äºæœ€å¤§è¯­ä¹‰å¥–åŠ±(çº¦0.1)

---

## ğŸ› ï¸ è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: ç¼©æ”¾æƒ©ç½šé¡¹ï¼ˆæ¨èï¼‰

**è°ƒæ•´æƒ©ç½šç³»æ•°ï¼Œä½¿å…¶ä¸è¯­ä¹‰å¥–åŠ±åŒé‡çº§**

```python
# ä¿®æ”¹ Environment_marl_indoor.py __init__
def __init__(self, ..., 
             collision_penalty=-0.05,      # ä»-0.5æ”¹ä¸º-0.05 (é™ä½10å€)
             low_accuracy_penalty=-0.03,   # ä»-0.3æ”¹ä¸º-0.03 (é™ä½10å€)
             ...)
```

**é¢„æœŸæ•ˆæœ**:
- ç¢°æ’æƒ©ç½š: -0.05 â†’ å æ¯”çº¦3.5%
- ä½å‡†ç¡®åº¦æƒ©ç½š: -0.03 â†’ å æ¯”çº¦4.9%
- è¯­ä¹‰å¥–åŠ±: 0.04 â†’ å æ¯”çº¦50%+

### æ–¹æ¡ˆ2: æ”¾å¤§è¯­ä¹‰å¥–åŠ±

**åœ¨å¥–åŠ±è®¡ç®—ä¸­æ”¾å¤§è¯­ä¹‰EE**

```python
# åœ¨ act_for_training ä¸­
if successful_count > 0:
    reward = semantic_EE_sum / self.n_Veh * 10.0  # æ”¾å¤§10å€
    semantic_accuracy_reward = semantic_accuracy_reward / self.n_Veh * 10.0
```

**ä¼˜ç‚¹**: ä¸æ”¹å˜æƒ©ç½šçš„ç»å¯¹æ„ä¹‰
**ç¼ºç‚¹**: å¯èƒ½ä½¿å¥–åŠ±è¿‡å¤§

### æ–¹æ¡ˆ3: æ··åˆæ–¹æ¡ˆï¼ˆæœ€æ¨èï¼‰

**åŒæ—¶è°ƒæ•´æƒ©ç½šå’Œå¥–åŠ±çš„ç¼©æ”¾**

```python
# è°ƒæ•´å‚æ•°
collision_penalty = -0.1       # ä»-0.5æ”¹ä¸º-0.1 (é™ä½5å€)
low_accuracy_penalty = -0.05   # ä»-0.3æ”¹ä¸º-0.05 (é™ä½6å€)

# æ”¾å¤§è¯­ä¹‰å¥–åŠ±
semantic_accuracy_reward *= 5.0  # æ”¾å¤§5å€
```

**é¢„æœŸæ•ˆæœ**:
- è¯­ä¹‰å¥–åŠ±: 0.04 * 5 = 0.2 â†’ å æ¯”çº¦50%
- ç¢°æ’æƒ©ç½š: -0.1 / 6 â‰ˆ -0.017 â†’ å æ¯”çº¦4%
- ä½å‡†ç¡®åº¦æƒ©ç½š: -0.05 / 6 â‰ˆ -0.008 â†’ å æ¯”çº¦2%

### æ–¹æ¡ˆ4: å¥–åŠ±å½’ä¸€åŒ–ï¼ˆæ ‡å‡†åŒ–ï¼‰

**ä½¿ç”¨running mean/stdè¿›è¡Œåœ¨çº¿å½’ä¸€åŒ–**

```python
class RewardNormalizer:
    def __init__(self, gamma=0.99):
        self.mean = 0.0
        self.var = 1.0
        self.count = 0
        self.gamma = gamma
    
    def normalize(self, reward):
        # æ›´æ–°ç»Ÿè®¡é‡
        self.count += 1
        delta = reward - self.mean
        self.mean += delta / self.count
        self.var = self.gamma * self.var + (1 - self.gamma) * delta ** 2
        
        # å½’ä¸€åŒ–
        std = np.sqrt(self.var + 1e-8)
        return (reward - self.mean) / std

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
reward_normalizer = RewardNormalizer()
normalized_reward = reward_normalizer.normalize(train_reward)
```

---

## ğŸ“Š æ¨èæ–¹æ¡ˆ

### ç«‹å³å®æ–½ï¼šæ–¹æ¡ˆ3ï¼ˆæ··åˆæ–¹æ¡ˆï¼‰

**ä¿®æ”¹å‚æ•°**:

1. **é™ä½æƒ©ç½šç³»æ•°** (åœ¨arguments.pyå’Œç¯å¢ƒåˆå§‹åŒ–)
   ```python
   parser.add_argument('--collision_penalty', type=float, default=-0.1)    # ä»-0.5æ”¹ä¸º-0.1
   parser.add_argument('--low_accuracy_penalty', type=float, default=-0.05) # ä»-0.3æ”¹ä¸º-0.05
   ```

2. **æ”¾å¤§è¯­ä¹‰å¥–åŠ±** (åœ¨Environment_marl_indoor.pyçš„act_for_training)
   ```python
   # åœ¨å½’ä¸€åŒ–å
   semantic_accuracy_reward = semantic_accuracy_reward / self.n_Veh * 5.0  # æ”¾å¤§5å€
   ```

3. **è°ƒæ•´åŠŸç‡æƒ©ç½šç³»æ•°**
   ```python
   power_penalty -= transmission_power_linear[i] * 0.0001  # ä»0.001æ”¹ä¸º0.0001 (é™ä½10å€)
   ```

**é¢„æœŸæ–°çš„å æ¯”**:
- è¯­ä¹‰å‡†ç¡®åº¦å¥–åŠ±: **çº¦50-60%** âœ…
- ç¢°æ’æƒ©ç½š: çº¦15-20%
- ä½å‡†ç¡®åº¦æƒ©ç½š: çº¦10-15%
- åŠŸç‡æƒ©ç½š: çº¦5-10%

---

## ğŸ¯ å®æ–½æ­¥éª¤

### Step 1: ä¿®æ”¹arguments.py

```python
parser.add_argument('--collision_penalty', type=float, default=-0.1, 
                    help='Penalty for RB collision (scaled down from -0.5)')
parser.add_argument('--low_accuracy_penalty', type=float, default=-0.05, 
                    help='Penalty for low semantic accuracy (scaled down from -0.3)')
```

### Step 2: ä¿®æ”¹Environment_marl_indoor.py

åœ¨`act_for_training`å‡½æ•°ä¸­:

```python
# åŸæ¥:
# semantic_accuracy_reward = semantic_accuracy_reward / self.n_Veh

# ä¿®æ”¹ä¸º:
SEMANTIC_REWARD_SCALE = 5.0  # æ”¾å¤§å› å­
semantic_accuracy_reward = semantic_accuracy_reward / self.n_Veh * SEMANTIC_REWARD_SCALE

# åŸæ¥:
# power_penalty -= transmission_power_linear[i] * 0.001

# ä¿®æ”¹ä¸º:
POWER_PENALTY_SCALE = 0.0001  # é™ä½åŠŸç‡æƒ©ç½š
power_penalty -= transmission_power_linear[i] * POWER_PENALTY_SCALE
```

### Step 3: é‡æ–°è®­ç»ƒå¹¶éªŒè¯

```bash
# åœæ­¢å½“å‰è®­ç»ƒ
# ä¿®æ”¹ä»£ç 
# é‡æ–°å¼€å§‹è®­ç»ƒ
python main_PPO_AC.py
```

### Step 4: ç›‘æ§æ–°çš„å¥–åŠ±åˆ†å¸ƒ

åœ¨TensorBoardä¸­æŸ¥çœ‹:
- `Reward/Semantic_Accuracy_Reward` åº”è¯¥åœ¨0.1-0.5èŒƒå›´
- `Reward/Collision_Penalty` åº”è¯¥åœ¨-0.05èŒƒå›´
- `Reward/Low_Accuracy_Penalty` åº”è¯¥åœ¨-0.02èŒƒå›´
- æ€»å¥–åŠ±åº”è¯¥åœ¨-2åˆ°+2èŒƒå›´ï¼Œæ›´å®¹æ˜“è¾¾åˆ°æ­£å€¼

---

## ğŸ“ é¢„æœŸæ”¹å–„

### è®­ç»ƒæ”¶æ•›æ€§

1. **Agentæ›´å€¾å‘äºæ¢ç´¢æˆåŠŸç­–ç•¥**
   - è¯­ä¹‰å¥–åŠ±å æ¯”æé«˜ï¼ŒæˆåŠŸä¼ è¾“çš„å›æŠ¥æ›´æ˜æ˜¾
   - Agentæœ‰åŠ¨åŠ›å»ä¼˜åŒ–è¯­ä¹‰å‡†ç¡®åº¦

2. **å‡å°‘è¿‡åº¦ä¿å®ˆ**
   - æƒ©ç½šé™ä½ï¼ŒAgentä¸ä¼šè¿‡åº¦å®³æ€•ç¢°æ’
   - æ›´æ„¿æ„å°è¯•ä¸åŒçš„RBåˆ†é…

3. **å¥–åŠ±ä¿¡å·æ›´æ¸…æ™°**
   - å„ç»„ä»¶æ¯”ä¾‹å¹³è¡¡ï¼Œå­¦ä¹ ä¿¡å·æ›´æ˜ç¡®
   - é¿å…è¢«æƒ©ç½šé¡¹ä¸»å¯¼

### UEå¹³è¡¡æ€§

- æ‰€æœ‰UEçš„å¥–åŠ±ä¿¡å·æ›´å¹³è¡¡
- å·®çš„UEä¹Ÿèƒ½å¾—åˆ°è¶³å¤Ÿçš„å­¦ä¹ ä¿¡å·
- å¯èƒ½æ”¹å–„UE0/UE2/UE5çš„æˆåŠŸç‡

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **éœ€è¦é‡æ–°è®­ç»ƒ**
   - ä¿®æ”¹å¥–åŠ±å‡½æ•°åï¼Œä¹‹å‰çš„æ¨¡å‹å¯èƒ½ä¸é€‚ç”¨
   - å»ºè®®ä»å¤´å¼€å§‹è®­ç»ƒ

2. **å¯èƒ½éœ€è¦å¾®è°ƒ**
   - å¦‚æœæ•ˆæœä¸ç†æƒ³ï¼Œå¯ä»¥è°ƒæ•´æ”¾å¤§/ç¼©å°å› å­
   - å»ºè®®çš„èŒƒå›´:
     - `SEMANTIC_REWARD_SCALE`: 3.0-10.0
     - `collision_penalty`: -0.05 to -0.2
     - `low_accuracy_penalty`: -0.03 to -0.1

3. **ç›‘æ§è®­ç»ƒ**
   - å¯†åˆ‡å…³æ³¨å‰100ä¸ªepisodeçš„å¥–åŠ±åˆ†å¸ƒ
   - ç¡®ä¿è¯­ä¹‰å¥–åŠ±å æ¯”åœ¨40-60%

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: 2025-12-10  
**ä¼˜å…ˆçº§**: ğŸ”´ **é«˜** - è¿™æ˜¯å¯¼è‡´è®­ç»ƒä¸æ”¶æ•›çš„ä¸»è¦åŸå› 

