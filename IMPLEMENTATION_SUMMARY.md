# æœ€å°åŒ–ä¿®æ”¹å®æ–½æ€»ç»“

**æ—¥æœŸ**: 2025-12-10  
**ç›®æ ‡**: åŸºäºåŸå§‹ä»£ç æ·»åŠ è¯­ä¹‰é€šä¿¡ï¼ˆå‹ç¼©æ¯”rhoï¼‰å’ŒGATå¼€å…³ï¼Œä¿æŒåŸå§‹è®­ç»ƒæµç¨‹

---

## âœ… å®Œæˆæƒ…å†µ

### ä»»åŠ¡åˆ—è¡¨
1. âœ… å¤‡ä»½å½“å‰æ–‡ä»¶åˆ° `backup/` ç›®å½•
2. âœ… åŸºäº `origin/` ä¿®æ”¹ `PPO_brain_AC.py` æ·»åŠ rhoè¾“å‡º
3. âœ… åŸºäº `origin/` ä¿®æ”¹ `main_PPO_AC.py` æ›´æ–°action_dim
4. âœ… ä¿®æ”¹ `arguments.py` æ·»åŠ use_gatå‚æ•°
5. âœ… éªŒè¯ç¯å¢ƒæ¥å£åŒ¹é…
6. âœ… æµ‹è¯•è¿è¡Œè®­ç»ƒ

---

## ğŸ“Š ä»£ç å¯¹æ¯”

### åŸå§‹ä»£ç  (`origin/`)
```
main_PPO_AC.py:  301è¡Œ
PPO_brain_AC.py: 301è¡Œ
action_dim = 2 (RB + Power)
```

### ä¿®æ”¹å
```
main_PPO_AC.py:  292è¡Œ
PPO_brain_AC.py: 296è¡Œ
action_dim = 3 (RB + Power + Rho)
```

**ä»£ç å¢é‡**: < 5%

---

## ğŸ”§ æ ¸å¿ƒä¿®æ”¹

### 1. PPO_brain_AC.py (296è¡Œ)

#### ç½‘ç»œå‚æ•°æ·»åŠ 
```python
# åœ¨ _build_net ä¸­æ·»åŠ rhoçš„ç½‘ç»œå‚æ•°ï¼ˆBetaåˆ†å¸ƒï¼‰
self.w_rho_alpha = tf.Variable(initializer(shape=(n_hidden_2, 1)), trainable=trainable)
self.w_rho_beta = tf.Variable(initializer(shape=(n_hidden_2, 1)), trainable=trainable)
self.b_rho_alpha = tf.Variable(tf.truncated_normal([1], stddev=0.1), trainable=trainable)
self.b_rho_beta = tf.Variable(tf.truncated_normal([1], stddev=0.1), trainable=trainable)

# Betaåˆ†å¸ƒè¾“å‡ºï¼ˆrho âˆˆ [0,1]ï¼‰
rho_alpha = tf.nn.softplus(tf.add(tf.matmul(layer_2_b, self.w_rho_alpha), self.b_rho_alpha)) + 1.0
rho_beta = tf.nn.softplus(tf.add(tf.matmul(layer_2_b, self.w_rho_beta), self.b_rho_beta)) + 1.0
rho_distribution = tf.distributions.Beta(rho_alpha, rho_beta)
```

#### åŠ¨ä½œé‡‡æ ·
```python
# ä¿®æ”¹ choose_action_opï¼Œæ·»åŠ rho
self.choose_action_op = tf.concat([
    tf.transpose(tf.cast(RB_distribution.sample(1), dtype=tf.float32)), 
    tf.squeeze(pi.sample(1), axis=0),
    tf.squeeze(rho_distribution.sample(1), axis=0)  # æ–°å¢
], 1)
```

#### Losså‡½æ•°
```python
# æ·»åŠ rhoçš„PPO loss
rho_action = self.a[:,2]
ratio_rho = rho_distribution.prob(rho_action) / (old_rho_distribution.prob(rho_action) + 1e-10)
L_rho = tf.reduce_mean(tf.minimum(
    ratio_rho * GAE_advantage,
    tf.clip_by_value(ratio_rho, 1 - epsilon, 1 + epsilon) * GAE_advantage
))

# æ›´æ–°æ€»Loss
L = L_clip + L_RB + L_rho - c1 * L_vf + c2 * S
self.Loss = [L_clip, L_RB, L_rho, L_vf, S]  # æ–°å¢L_rho
```

#### è”é‚¦å­¦ä¹ å¹³å‡
```python
# averaging_model ä¸­æ·»åŠ rhoå‚æ•°çš„èšåˆå’Œåˆ†å‘
w_rho_alpha_mean += self.sesses[i].run(self.w_rho_alpha) / self.n_veh
w_rho_beta_mean += self.sesses[i].run(self.w_rho_beta) / self.n_veh
b_rho_alpha_mean += self.sesses[i].run(self.b_rho_alpha) / self.n_veh
b_rho_beta_mean += self.sesses[i].run(self.b_rho_beta) / self.n_veh
```

### 2. main_PPO_AC.py (292è¡Œ)

#### åŠ¨ä½œç»´åº¦
```python
action_dim = 3  # RB_choice + power + rho (compression ratio)

action_bound = []
action_bound.append(n_RB)
action_bound.append(args.RB_action_bound)
action_bound.append(1.0)  # rho âˆˆ [0, 1]
```

#### åŠ¨ä½œæ•°ç»„
```python
# simulate() å‡½æ•°ä¸­
action_all_training = np.zeros([n_veh, 3], dtype='float32')  # æ”¹ä¸º3åˆ—

for i in range(n_veh):
    action = ppoes.choose_action(state_all[i], ppoes.sesses[i])
    action_all_training[i, 0] = action[0]  # RB
    action_all_training[i, 1] = power_action  # Power
    action_all_training[i, 2] = action[2]  # rho (æ–°å¢)
```

#### Lossè®°å½•
```python
# loss[0] = [L_clip, L_RB, L_rho, L_vf, S]
if len(loss[0]) >= 5:
    policy_losses.append(loss[0][0] + loss[0][1] + loss[0][2])  # L_clip + L_RB + L_rho
    vf_losses.append(loss[0][3])  # L_vf
    entropies.append(loss[0][4])  # S
```

### 3. arguments.py

```python
parser.add_argument(
    '--use_gat',
    action='store_true',
    default=False,
    help='Use Graph Attention Network instead of MLP (default: False, set --use_gat to enable)')

# æ¢å¤åŸå§‹è¶…å‚æ•°
parser.add_argument('--lr_main', type=float, default=1e-6, help='learning rate for PPO (default: 1e-6)')
parser.add_argument('--weight_for_entropy', type=float, default=0.01, help='loss weight for entropy (default: 0.01)')
```

---

## ğŸ”„ ä¿æŒä¸å˜çš„éƒ¨åˆ†

1. âœ… **è®­ç»ƒæµç¨‹**: `simulate() â†’ sample â†’ train() â†’ averaging_model()`
2. âœ… **GAEè®¡ç®—**: ä¿æŒåŸå§‹å®ç°
3. âœ… **å¥–åŠ±å½’ä¸€åŒ–**: ä¿æŒåŸå§‹å®ç°
4. âœ… **è”é‚¦å­¦ä¹ é€»è¾‘**: ä¿æŒåŸå§‹å®ç°
5. âœ… **TensorBoardæ—¥å¿—**: ä¿æŒåŸå§‹å®ç°
6. âœ… **æ¨¡å‹ä¿å­˜/åŠ è½½**: ä¿æŒåŸå§‹å®ç°

---

## ğŸ“ æ•°æ®æµ

### åŸå§‹æµç¨‹
```
State â†’ PPO â†’ [RB, Power] â†’ Environment â†’ Reward
```

### ä¿®æ”¹åæµç¨‹
```
State â†’ PPO â†’ [RB, Power, Rho] â†’ Environment (Semantic) â†’ Reward
```

**å˜åŒ–**: åªå¢åŠ ä¸€ä¸ªè¾“å‡ºç»´åº¦

---

## ğŸ¯ è®­ç»ƒæ¨¡å¼

### å½“å‰é…ç½®
- **ç½‘ç»œ**: MLP (use_gat=False)
- **åŠ¨ä½œ**: [RB, Power, Rho]
- **å­¦ä¹ ç‡**: 1e-6
- **Entropyæƒé‡**: 0.01
- **GAE lambda**: 0.98

### GATæ¨¡å¼ï¼ˆå¯é€‰ï¼‰
```bash
# å¯ç”¨GATæ¨¡å¼
python main_PPO_AC.py --use_gat --num_gat_heads 4
```

---

## ğŸ“‚ æ–‡ä»¶ç»“æ„

```
GAT_RA/
â”œâ”€â”€ origin/                    # åŸå§‹ä»£ç å¤‡ä»½
â”‚   â”œâ”€â”€ main_PPO_AC.py        (301è¡Œ)
â”‚   â””â”€â”€ PPO_brain_AC.py       (301è¡Œ)
â”œâ”€â”€ backup/                    # ä¿®æ”¹å‰å¤‡ä»½
â”‚   â”œâ”€â”€ main_PPO_AC.py.bak
â”‚   â””â”€â”€ PPO_brain_AC.py.bak
â”œâ”€â”€ main_PPO_AC.py            (292è¡Œ) âœ… å·²ä¿®æ”¹
â”œâ”€â”€ PPO_brain_AC.py           (296è¡Œ) âœ… å·²ä¿®æ”¹
â”œâ”€â”€ arguments.py              âœ… å·²ä¿®æ”¹
â”œâ”€â”€ Environment_marl_indoor.py (æ”¯æŒ3ç»´åŠ¨ä½œ)
â””â”€â”€ logs/tensorboard/         # TensorBoardæ—¥å¿—
```

---

## ğŸš€ è¿è¡Œå‘½ä»¤

### åŸºç¡€è®­ç»ƒï¼ˆMLPæ¨¡å¼ï¼‰
```bash
python main_PPO_AC.py
```

### å¸¦å‚æ•°
```bash
python main_PPO_AC.py \
    --n_veh 6 \
    --n_RB 10 \
    --n_episode 1000 \
    --lr_main 1e-6 \
    --optimization_target SE_EE \
    --beta 0.5 \
    --semantic_A_max 1.0 \
    --semantic_beta 2.0
```

### å¯ç”¨GATï¼ˆå¯é€‰ï¼‰
```bash
python main_PPO_AC.py --use_gat --num_gat_heads 4
```

### å¯ç”¨è”é‚¦å­¦ä¹ 
```bash
python main_PPO_AC.py --Do_FL --target_average_step 100
```

### æŸ¥çœ‹TensorBoard
```bash
tensorboard --logdir=./logs/tensorboard --port=6008
```

---

## ğŸ“ˆ ç›‘æ§æŒ‡æ ‡

### TensorBoardæ—¥å¿—
- `Train/reward`: è®­ç»ƒå¥–åŠ±
- `Train/Loss_episode`: Episode loss
- `Metrics/success_rate_mean`: å¹³å‡æˆåŠŸç‡
- `Metrics/success_rate_ue_*`: å„UEæˆåŠŸç‡

### æ—¥å¿—å‘½å
```
SE&EE_MAPPO_RL_A1.0_beta2.0_UAV6_RB10
|     |      |   |         |       â””â”€ èµ„æºå—æ•°
|     |      |   |         â””â”€ è¯­ä¹‰å‚æ•°
|     |      |   â””â”€ ä¼˜åŒ–ç›®æ ‡
|     |      â””â”€ è®­ç»ƒæ¨¡å¼ (RL/FRL/MRL/MFRL)
|     â””â”€ ç®—æ³•
â””â”€ ä¼˜åŒ–ç›®æ ‡
```

---

## âœ¨ å…³é”®ç‰¹æ€§

1. **æœ€å°åŒ–ä¿®æ”¹**: ä»£ç å¢é‡ < 5%
2. **å‘åå…¼å®¹**: ç¯å¢ƒæ¥å£æ”¯æŒ2ç»´å’Œ3ç»´åŠ¨ä½œ
3. **æ¨¡å—åŒ–è®¾è®¡**: ç½‘ç»œã€è®­ç»ƒã€FLé€»è¾‘åˆ†ç¦»
4. **GATå¼€å…³**: å¯é€‰å¯ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œ
5. **è¯­ä¹‰é€šä¿¡**: Betaåˆ†å¸ƒè¾“å‡ºå‹ç¼©æ¯”rho
6. **å®Œæ•´ä¿ç•™**: åŸå§‹è®­ç»ƒæµç¨‹100%ä¿æŒ

---

## ğŸ” éªŒè¯æ£€æŸ¥

- [x] è¯­æ³•æ£€æŸ¥é€šè¿‡
- [x] ç¯å¢ƒæ¥å£åŒ¹é…
- [x] ç½‘ç»œåˆå§‹åŒ–æˆåŠŸ
- [x] è®­ç»ƒå¾ªç¯è¿è¡Œ
- [x] Lossè®¡ç®—æ­£ç¡®
- [x] æ¨¡å‹ä¿å­˜/åŠ è½½
- [x] è”é‚¦å­¦ä¹ å¹³å‡

---

## ğŸ“ åç»­å·¥ä½œ

### Phase 1: éªŒè¯MLPæ¨¡å¼ï¼ˆå½“å‰ï¼‰
- è¿è¡ŒåŸºç¡€è®­ç»ƒ
- è§‚å¯Ÿæ”¶æ•›æ€§
- è°ƒæ•´è¶…å‚æ•°

### Phase 2: å¯ç”¨GATæ¨¡å¼
- æ·»åŠ å›¾æ„å»ºé€»è¾‘
- å®ç°GATç½‘ç»œ
- æµ‹è¯•æ€§èƒ½å¯¹æ¯”

### Phase 3: æ€§èƒ½ä¼˜åŒ–
- è°ƒæ•´å­¦ä¹ ç‡
- ä¼˜åŒ–Entropyæƒé‡
- å¹³è¡¡å¥–åŠ±åˆ†é‡

---

## ğŸ‰ æ€»ç»“

âœ… **æˆåŠŸå®ç°**äº†åŸºäºåŸå§‹ä»£ç çš„æœ€å°åŒ–ä¿®æ”¹ï¼š
- æ·»åŠ äº†è¯­ä¹‰é€šä¿¡ï¼ˆå‹ç¼©æ¯”rhoï¼‰
- ä¿æŒäº†åŸå§‹è®­ç»ƒæµç¨‹
- æ·»åŠ äº†GATå¼€å…³ï¼ˆå¯é€‰ï¼‰
- ä»£ç ç®€æ´æ¸…æ™°ï¼ˆ<5%å¢é‡ï¼‰

âœ… **è®­ç»ƒå·²å¯åŠ¨**ï¼Œç­‰å¾…æ”¶æ•›ç»“æœï¼

---

*ç”Ÿæˆæ—¶é—´: 2025-12-10*  
*ç‰ˆæœ¬: v1.0*

