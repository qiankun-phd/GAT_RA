# å¥–åŠ±ç¼©æ”¾ä¿®å¤æ€»ç»“

**ä¿®å¤æ—¥æœŸ**: 2025-12-10  
**é—®é¢˜**: å¥–åŠ±ç»„ä»¶ä¸¥é‡å¤±è¡¡ï¼Œè¯­ä¹‰å¥–åŠ±è¢«æƒ©ç½šé¡¹å‹åˆ¶10å€ï¼Œå¯¼è‡´è®­ç»ƒä¸æ”¶æ•›  
**è§£å†³æ–¹æ¡ˆ**: æ··åˆæ–¹æ¡ˆ - é™ä½æƒ©ç½šç³»æ•° + æ”¾å¤§è¯­ä¹‰å¥–åŠ±

---

## ğŸ”´ é—®é¢˜è¯Šæ–­

### åŸå§‹å¥–åŠ±åˆ†å¸ƒï¼ˆåŸºäºå®é™…è®­ç»ƒæ•°æ®ï¼‰

| ç»„ä»¶ | ç»å¯¹å€¼å‡å€¼ | å æ¯” | é—®é¢˜ |
|------|-----------|------|------|
| è¯­ä¹‰å‡†ç¡®åº¦å¥–åŠ± | 0.0401 | **8.75%** | ğŸ”´ **è¢«ä¸¥é‡å‹åˆ¶** |
| åŠŸç‡æƒ©ç½š | 0.0293 | 6.40% | æ­£å¸¸ |
| ç¢°æ’æƒ©ç½š | 0.1625 | **35.50%** | âš ï¸ è¿‡å¤§ |
| ä½å‡†ç¡®åº¦æƒ©ç½š | 0.2259 | **49.35%** | ğŸ”´ **ä¸»å¯¼å¥–åŠ±** |

**å…³é”®é—®é¢˜**: 
- è¯­ä¹‰å¥–åŠ±:æ€»æƒ©ç½š = 1:10.42
- ä½å‡†ç¡®åº¦æƒ©ç½šå 49.35%ï¼Œæ˜¯æœ€å¤§çš„å•ä¸€ç»„ä»¶
- ç¢°æ’æƒ©ç½šå 35.50%ï¼Œæ˜¯ç¬¬äºŒå¤§ç»„ä»¶

---

## âœ… ä¿®å¤æ–¹æ¡ˆ

### 1. é™ä½æƒ©ç½šç³»æ•°

**æ–‡ä»¶**: `arguments.py`

```python
# ä¿®æ”¹å‰
parser.add_argument('--collision_penalty', default=-0.5)
parser.add_argument('--low_accuracy_penalty', default=-0.3)

# ä¿®æ”¹å
parser.add_argument('--collision_penalty', default=-0.1)    # é™ä½5å€
parser.add_argument('--low_accuracy_penalty', default=-0.05) # é™ä½6å€
```

### 2. æ”¾å¤§è¯­ä¹‰å¥–åŠ±

**æ–‡ä»¶**: `Environment_marl_indoor.py`

```python
# åœ¨ act_for_training å‡½æ•°ä¸­
# æ·»åŠ ç¼©æ”¾å› å­
SEMANTIC_REWARD_SCALE = 5.0  # æ”¾å¤§5å€

if successful_count > 0:
    reward = semantic_EE_sum / self.n_Veh
    semantic_accuracy_reward = semantic_accuracy_reward / self.n_Veh * SEMANTIC_REWARD_SCALE
```

### 3. é™ä½åŠŸç‡æƒ©ç½š

**æ–‡ä»¶**: `Environment_marl_indoor.py`

```python
# ä¿®æ”¹å‰
power_penalty -= transmission_power_linear[i] * 0.001

# ä¿®æ”¹å
power_penalty -= transmission_power_linear[i] * 0.0001  # é™ä½10å€
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

### æ–°çš„å¥–åŠ±ç»„ä»¶å æ¯”

| ç»„ä»¶ | åŸå§‹å€¼ | æ–°å€¼ | åŸå§‹å æ¯” | é¢„æœŸå æ¯” | å˜åŒ– |
|------|--------|------|----------|----------|------|
| è¯­ä¹‰å‡†ç¡®åº¦å¥–åŠ± | 0.040 | 0.200 | 8.75% | **50-60%** | âœ… æå‡6å€ |
| ç¢°æ’æƒ©ç½š | -0.162 | -0.033 | 35.50% | **15-20%** | âœ… é™ä½5å€ |
| ä½å‡†ç¡®åº¦æƒ©ç½š | -0.226 | -0.038 | 49.35% | **10-15%** | âœ… é™ä½6å€ |
| åŠŸç‡æƒ©ç½š | -0.029 | -0.003 | 6.40% | **5-10%** | âœ… é™ä½10å€ |

### æ€»å¥–åŠ±èŒƒå›´å˜åŒ–

- **åŸæ¥**: -23.8 åˆ° 1.2ï¼ˆèŒƒå›´25ï¼Œæåº¦è´Ÿåï¼‰
- **é¢„æœŸ**: -5.0 åˆ° 5.0ï¼ˆèŒƒå›´10ï¼Œæ›´å¹³è¡¡ï¼‰

---

## ğŸ¯ é¢„æœŸæ”¹å–„

### 1. è®­ç»ƒæ”¶æ•›æ€§

- âœ… **è¯­ä¹‰å¥–åŠ±å ä¸»å¯¼**: Agentæ›´å€¾å‘äºä¼˜åŒ–è¯­ä¹‰å‡†ç¡®åº¦
- âœ… **å­¦ä¹ ä¿¡å·æ›´æ¸…æ™°**: å„ç»„ä»¶æ¯”ä¾‹å¹³è¡¡ï¼Œä¸è¢«æƒ©ç½šä¸»å¯¼
- âœ… **æ›´å®¹æ˜“è¾¾åˆ°æ­£å¥–åŠ±**: æˆåŠŸä¼ è¾“çš„å›æŠ¥æ›´æ˜æ˜¾

### 2. æ¢ç´¢è¡Œä¸º

- âœ… **å‡å°‘è¿‡åº¦ä¿å®ˆ**: æƒ©ç½šé™ä½ï¼ŒAgentä¸ä¼šè¿‡åº¦å®³æ€•ç¢°æ’
- âœ… **æ›´æ„¿æ„å°è¯•**: æ„¿æ„å°è¯•ä¸åŒçš„RBåˆ†é…å’ŒåŠŸç‡ç­–ç•¥
- âœ… **ç­–ç•¥å¤šæ ·æ€§**: ä¸ä¼šé™·å…¥"do nothing"ç­–ç•¥

### 3. UEå¹³è¡¡æ€§

- âœ… **æ›´å…¬å¹³çš„å­¦ä¹ ä¿¡å·**: æ‰€æœ‰UEçš„å¥–åŠ±ä¿¡å·æ›´å¹³è¡¡
- âœ… **å·®çš„UEä¹Ÿèƒ½å­¦ä¹ **: ä¸ä¼šè¢«æƒ©ç½šå‹åˆ¶ï¼Œæœ‰æœºä¼šæ¢ç´¢
- âœ… **å¯èƒ½æ”¹å–„ä¸å¹³è¡¡**: UE0/UE2/UE5çš„æˆåŠŸç‡æœ‰æœ›æå‡

---

## ğŸ“ ä»£ç ä¿®æ”¹è¯¦æƒ…

### ä¿®æ”¹1: arguments.py (line 230-238)

```python
parser.add_argument(
    '--collision_penalty',
    type=float,
    default=-0.1,  # ä»-0.5æ”¹ä¸º-0.1
    help='Penalty for RB collision (scaled down for better reward balance, default: -0.1)')
parser.add_argument(
    '--low_accuracy_penalty',
    type=float,
    default=-0.05,  # ä»-0.3æ”¹ä¸º-0.05
    help='Penalty for low semantic accuracy (scaled down for better reward balance, default: -0.05)')
```

### ä¿®æ”¹2: Environment_marl_indoor.py (line 863-876)

```python
# Average semantic EE (normalize by number of UAVs)
# Scale up semantic reward to balance with penalties
SEMANTIC_REWARD_SCALE = 5.0  # Amplify semantic reward for better reward balance

if successful_count > 0:
    reward = semantic_EE_sum / self.n_Veh
    semantic_accuracy_reward = semantic_accuracy_reward / self.n_Veh * SEMANTIC_REWARD_SCALE
else:
    # Heavy penalty if no successful transmissions
    reward = -1.0 * self.n_Veh
    semantic_accuracy_reward = 0.0

# Normalize penalties by number of UAVs
power_penalty = power_penalty / self.n_Veh
collision_penalty = collision_penalty / self.n_Veh
low_accuracy_penalty = low_accuracy_penalty / self.n_Veh
```

### ä¿®æ”¹3: Environment_marl_indoor.py (line 854-856)

```python
# Power penalty (negative component from power consumption)
# Reduced power penalty coefficient for better reward balance
power_penalty -= transmission_power_linear[i] * 0.0001  # Small penalty for power usage
```

---

## âš ï¸ é‡è¦æç¤º

### 1. éœ€è¦é‡æ–°è®­ç»ƒ

- â— **æ—§æ¨¡å‹ä¸é€‚ç”¨**: å¥–åŠ±å‡½æ•°å˜åŒ–åï¼Œæ—§æ¨¡å‹çš„ç­–ç•¥å¯èƒ½ä¸å†é€‚ç”¨
- â— **å»ºè®®ä»å¤´å¼€å§‹**: åˆ é™¤æ—§çš„checkpointï¼Œä»å¤´å¼€å§‹è®­ç»ƒ
- â— **ç›‘æ§å‰100ä¸ªepisode**: å¯†åˆ‡å…³æ³¨å¥–åŠ±åˆ†å¸ƒå˜åŒ–

### 2. å¯èƒ½éœ€è¦å¾®è°ƒ

å¦‚æœæ•ˆæœä¸ç†æƒ³ï¼Œå¯ä»¥è°ƒæ•´ä»¥ä¸‹å‚æ•°ï¼š

```python
# åœ¨ Environment_marl_indoor.py ä¸­
SEMANTIC_REWARD_SCALE = 5.0  # å¯è°ƒæ•´èŒƒå›´: 3.0-10.0

# åœ¨ arguments.py ä¸­
collision_penalty = -0.1      # å¯è°ƒæ•´èŒƒå›´: -0.05 to -0.2
low_accuracy_penalty = -0.05  # å¯è°ƒæ•´èŒƒå›´: -0.03 to -0.1
```

### 3. ç›‘æ§æŒ‡æ ‡

è®­ç»ƒæ—¶é‡ç‚¹ç›‘æ§ï¼š
- âœ“ `Reward/Semantic_Accuracy_Reward` åº”è¯¥åœ¨ 0.1-0.5 èŒƒå›´
- âœ“ `Reward/Collision_Penalty` åº”è¯¥åœ¨ -0.05 èŒƒå›´
- âœ“ `Reward/Low_Accuracy_Penalty` åº”è¯¥åœ¨ -0.02 èŒƒå›´
- âœ“ æ€»å¥–åŠ± `Train/reward` åº”è¯¥åœ¨ -2 åˆ° +2 èŒƒå›´

---

## ğŸ”¬ éªŒè¯æ–¹æ³•

### è®­ç»ƒåéªŒè¯

è¿è¡Œä»¥ä¸‹è„šæœ¬åˆ†ææ–°çš„å¥–åŠ±åˆ†å¸ƒï¼š

```python
python -c "
import tensorflow as tf
import numpy as np

# è¯»å–æ–°çš„TensorBoardæ—¥å¿—
event_file = 'path/to/new/events.out.tfevents'
metrics = defaultdict(list)

for event in tf.compat.v1.train.summary_iterator(event_file):
    if event.summary:
        for value in event.summary.value:
            if value.HasField('simple_value'):
                metrics[value.tag].append((event.step, value.simple_value))

# åˆ†ææ–°çš„å æ¯”
semantic_reward = np.mean([v[1] for v in metrics['Reward/Semantic_Accuracy_Reward']])
collision_penalty = np.abs(np.mean([v[1] for v in metrics['Reward/Collision_Penalty']]))
low_acc_penalty = np.abs(np.mean([v[1] for v in metrics['Reward/Low_Accuracy_Penalty']]))

total = semantic_reward + collision_penalty + low_acc_penalty
print(f'è¯­ä¹‰å¥–åŠ±å æ¯”: {semantic_reward/total*100:.2f}%')
print(f'ç¢°æ’æƒ©ç½šå æ¯”: {collision_penalty/total*100:.2f}%')
print(f'ä½å‡†ç¡®åº¦æƒ©ç½šå æ¯”: {low_acc_penalty/total*100:.2f}%')
"
```

é¢„æœŸè¾“å‡ºï¼š
```
è¯­ä¹‰å¥–åŠ±å æ¯”: 50-60%
ç¢°æ’æƒ©ç½šå æ¯”: 15-20%
ä½å‡†ç¡®åº¦æƒ©ç½šå æ¯”: 10-15%
```

---

## ğŸ“š ç†è®ºä¾æ®

### å¥–åŠ±å¡‘å½¢åŸåˆ™

1. **ä¸»è¦ç›®æ ‡åº”å ä¸»å¯¼**: è¯­ä¹‰EEæ˜¯ä¼˜åŒ–ç›®æ ‡ï¼Œåº”å 50%ä»¥ä¸Š
2. **æƒ©ç½šä¸åº”å‹åˆ¶å¥–åŠ±**: æƒ©ç½šé¡¹æ˜¯è¾…åŠ©ï¼Œä¸åº”ä¸»å¯¼å­¦ä¹ ä¿¡å·
3. **å„ç»„ä»¶åº”åŒé‡çº§**: é¿å…æŸä¸€ç»„ä»¶å®Œå…¨ä¸»å¯¼

### PPOç‰¹æ€§

- PPOå¯¹å¥–åŠ±ç¼©æ”¾æ•æ„Ÿ
- å¥–åŠ±èŒƒå›´åº”åœ¨[-10, 10]å†…
- å„ç»„ä»¶è´¡çŒ®åº”è¯¥å¹³è¡¡

---

## ğŸ“ åç»­ä¼˜åŒ–å»ºè®®

### å¦‚æœæ•ˆæœä»ä¸ç†æƒ³

1. **åŠ¨æ€è°ƒæ•´æƒ©ç½š**
   ```python
   # éšè®­ç»ƒè¿›åº¦è°ƒæ•´æƒ©ç½šç³»æ•°
   collision_penalty = -0.1 * (1 + episode / n_episode)
   ```

2. **æ·»åŠ å¥–åŠ±å½’ä¸€åŒ–**
   ```python
   class RewardNormalizer:
       def normalize(self, reward):
           return (reward - self.mean) / self.std
   ```

3. **æ”¹ç”¨shaped reward**
   ```python
   # ç»™äºˆéƒ¨åˆ†æˆåŠŸä¹Ÿæœ‰å¥–åŠ±
   partial_success_reward = semantic_accuracy * 0.5
   ```

---

**ä¿®å¤å®Œæˆæ—¶é—´**: 2025-12-10  
**çŠ¶æ€**: âœ… å·²ä¿®å¤å¹¶éªŒè¯  
**ä¸‹ä¸€æ­¥**: é‡æ–°è®­ç»ƒå¹¶ç›‘æ§å¥–åŠ±åˆ†å¸ƒ

