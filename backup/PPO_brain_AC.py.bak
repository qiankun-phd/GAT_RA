import numpy as np
import os
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import random
import copy
import math

from arguments import get_args
args = get_args()

os.environ['PYTHONHASHSEED'] = str(args.seed)
random.seed(args.seed)
np.random.seed(args.seed)
tf.set_random_seed(args.set_random_seed)


my_config = tf.ConfigProto()
my_config.gpu_options.allow_growth = True

n_hidden_1 = args.n_hidden_1
n_hidden_2 = args.n_hidden_2
n_hidden_3 = args.n_hidden_3

sigma_add = args.sigma_add


def graph_attention_layer(inputs, adj_matrix, num_heads=4, out_dim=None, activation=tf.nn.relu, 
                          dropout_rate=0.0, is_training=True, name='gat'):
    """
    Multi-head Graph Attention Layer (GAT) implementation in TensorFlow
    
    Args:
        inputs: [N, F_in] node features, where N is number of nodes, F_in is input feature dim
        adj_matrix: [N, N] adjacency matrix (can be weighted)
        num_heads: number of attention heads
        out_dim: output dimension per head (default: F_in)
        activation: activation function
        dropout_rate: dropout rate
        is_training: whether in training mode
        name: variable scope name
    
    Returns:
        output: [N, out_dim * num_heads] concatenated multi-head features
    """
    with tf.variable_scope(name):
        N = tf.shape(inputs)[0]  # Number of nodes
        F_in = inputs.get_shape()[-1].value  # Input feature dimension
        
        if out_dim is None:
            out_dim = F_in
        
        # Linear transformation for each head
        head_outputs = []
        
        for head in range(num_heads):
            with tf.variable_scope(f'head_{head}'):
                # Weight matrix: [F_in, out_dim]
                W = tf.get_variable(f'W_{head}', shape=[F_in, out_dim],
                                   initializer=tf.compat.v1.keras.initializers.glorot_uniform())
                
                # Attention weight vector: [2 * out_dim, 1]
                a = tf.get_variable(f'a_{head}', shape=[2 * out_dim, 1],
                                   initializer=tf.compat.v1.keras.initializers.glorot_uniform())
                
                # Transform input features: [N, out_dim]
                h = tf.matmul(inputs, W)  # [N, out_dim]
                
                # Compute attention coefficients
                # h_i: [N, out_dim], h_j: [N, out_dim]
                # We need to compute attention for all pairs (i, j)
                
                # Optimized attention computation
                # Instead of tiling, use efficient matrix operations
                # Compute attention scores using: e_ij = LeakyReLU(a^T [Wh_i || Wh_j])
                # This can be computed more efficiently as:
                # e = LeakyReLU(W_h^T a_1 + W_h^T a_2) where a = [a_1; a_2]
                
                # Split attention vector: a = [a_1; a_2] where each is [out_dim, 1]
                a_1 = a[:out_dim, :]  # [out_dim, 1]
                a_2 = a[out_dim:, :]  # [out_dim, 1]
                
                # Compute: Wh_i^T a_1 for all i: [N, 1]
                # and: Wh_j^T a_2 for all j: [1, N]
                # Then broadcast and add
                e_i = tf.matmul(h, a_1)  # [N, 1]
                e_j = tf.matmul(h, a_2)  # [N, 1]
                
                # Broadcast: [N, 1] + [1, N] = [N, N]
                e = e_i + tf.transpose(e_j)  # [N, N]
                
                # Apply LeakyReLU
                e = tf.nn.leaky_relu(e, alpha=0.2)
                
                # Apply mask (only attend to neighbors)
                # adj_matrix: [N, N], mask out non-neighbors
                mask = -1e9 * (1.0 - adj_matrix)  # Large negative value for non-neighbors
                e = e + mask
                
                # Softmax over neighbors
                attention = tf.nn.softmax(e, axis=1)  # [N, N]
                
                # Apply dropout
                if dropout_rate > 0.0:
                    attention = tf.layers.dropout(attention, rate=dropout_rate, training=is_training)
                
                # Aggregate neighbor features: [N, out_dim]
                h_out = tf.matmul(attention, h)  # [N, out_dim]
                
                # Apply activation
                if activation is not None:
                    h_out = activation(h_out)
                
                head_outputs.append(h_out)
        
        # Concatenate all heads: [N, out_dim * num_heads]
        output = tf.concat(head_outputs, axis=1)
        
        return output


def multi_layer_gat(node_features, adj_matrix, hidden_dims, num_heads=4, 
                    activation=tf.nn.relu, dropout_rate=0.0, is_training=True, name='multi_gat'):
    """
    Multi-layer GAT encoder
    
    Args:
        node_features: [N, F_in] node features
        adj_matrix: [N, N] adjacency matrix
        hidden_dims: list of hidden dimensions for each layer
        num_heads: number of attention heads per layer
        activation: activation function
        dropout_rate: dropout rate
        is_training: whether in training mode
        name: variable scope name
    
    Returns:
        output: [N, hidden_dims[-1] * num_heads] final node embeddings
    """
    with tf.variable_scope(name):
        x = node_features
        
        for i, hidden_dim in enumerate(hidden_dims):
            # For intermediate layers, we need to handle multi-head concatenation
            if i == 0:
                # First layer: input is original features
                x = graph_attention_layer(x, adj_matrix, num_heads=num_heads, 
                                         out_dim=hidden_dim, activation=activation,
                                         dropout_rate=dropout_rate, is_training=is_training,
                                         name=f'gat_layer_{i}')
            else:
                # Subsequent layers: input is concatenated multi-head features
                # Project to match input dimension for next GAT layer
                input_dim = x.get_shape()[-1].value
                if input_dim != hidden_dim * num_heads:
                    # Project concatenated features to match expected input
                    with tf.variable_scope(f'project_{i}'):
                        W_proj = tf.get_variable(f'W_proj', 
                                               shape=[input_dim, hidden_dim * num_heads],
                                               initializer=tf.compat.v1.keras.initializers.glorot_uniform())
                        x = tf.matmul(x, W_proj)
                        if activation is not None:
                            x = activation(x)
                
                # Apply GAT layer
                x = graph_attention_layer(x, adj_matrix, num_heads=num_heads,
                                         out_dim=hidden_dim, activation=activation,
                                         dropout_rate=dropout_rate, is_training=is_training,
                                         name=f'gat_layer_{i}')
        
        return x

class PPO(object):
    def __init__(self, s_dim, a_bound, c1, c2, epsilon, lr, meta_lr, K, n_veh, n_RB, IS_meta, meta_episode, 
                 use_gat=True, num_gat_heads=4, node_feature_dim=None):
        """
        Args:
            s_dim: state dimension (for backward compatibility, if use_gat=False)
            a_bound: action bounds [RB_bound, power_bound, compression_bound]
            c1, c2, epsilon, lr, meta_lr, K: PPO hyperparameters
            n_veh: number of UAVs (nodes in graph)
            n_RB: number of resource blocks
            IS_meta: whether to use meta learning
            meta_episode: meta episode number
            use_gat: whether to use GAT instead of MLP
            num_gat_heads: number of attention heads in GAT
            node_feature_dim: dimension of node features (CSI, location, etc.)
        """
        self.a_bound = a_bound
        self.K = K
        self.s_dim = s_dim  # Keep for backward compatibility
        self.a_dim = 3  # RB_choice + Power + Compression Ratio (rho)
        self.n_RB = n_RB
        self.n_veh = n_veh
        self.IS_meta = IS_meta
        self.gamma = args.gamma
        self.GAE_discount = args.lambda_advantage
        self.use_gat = use_gat
        self.num_gat_heads = num_gat_heads
        self.c1 = c1  # Value function loss weight
        self.c2 = c2  # Entropy weight
        
        # Store PPO hyperparameters for use in train() function
        self.c1 = c1  # Weight for value function loss
        self.c2 = c2  # Weight for entropy bonus
        self.epsilon = epsilon  # PPO clipping parameter
        self.lr = lr  # Learning rate
        self.meta_lr = meta_lr  # Meta learning rate
        
        # Graph-based input (for GAT)
        if use_gat:
            # Node features: [batch_size, n_veh, node_feature_dim]
            # Adjacency matrix: [batch_size, n_veh, n_veh]
            if node_feature_dim is None:
                # Default: CSI features (n_RB * 2) + position (3) + other features
                node_feature_dim = n_RB * 2 + 3 + 2  # CSI fast, CSI abs, position (x,y,z), success, episode
            self.node_feature_dim = node_feature_dim
            self.node_features = tf.placeholder(tf.float32, shape=(None, n_veh, node_feature_dim), name='node_features')
            self.adj_matrix = tf.placeholder(tf.float32, shape=(None, n_veh, n_veh), name='adj_matrix')
            # For single-agent processing, we also keep the old interface
            self.s_input = tf.placeholder(tf.float32, shape=(None, self.s_dim), name='s_t')
        else:
            # MLP-based input (backward compatibility)
            self.s_input = tf.placeholder(tf.float32, shape=(None, self.s_dim), name='s_t')

        # with tf.variable_scope('critic'):
        #     l1 = tf.layers.dense(self.tfs, 100, tf.nn.relu)
        #     self.v = tf.layers.dense(l1, 1)
        #     self.tfdc_r = tf.placeholder(tf.float32, [None, 1], 'discounted_r')
        #     self.advantage = self.tfdc_r - self.v
        #     self.closs = tf.reduce_mean(tf.square(self.advantage))
        #     self.ctrain_op = tf.train.AdamOptimizer(C_LR).minimize(self.closs)


        pi, RB_distribution, rho_distribution, self.v, params, self.saver = self._build_net('network', True)
        old_pi, old_RB_distribution, old_rho_distribution, old_v, old_params, _ = self._build_net('old_network', False)
        self.reward = tf.placeholder(tf.float32, shape=[None], name='reward')
        self.v_pred_next = tf.placeholder(dtype=tf.float32, shape=[None], name='v_pred_next')
        self.gae = tf.placeholder(dtype=tf.float32, shape=[None], name='gae')

        GAE_advantage = self.gae
        # GAE_advantage = get_gaes(self.discounted_r)

        self.a = tf.placeholder(tf.float32, shape=(None, self.a_dim), name='a_t')
        RB_action = self.a[:,0]
        power_action = self.a[:,1]
        rho_action = self.a[:,2]  # Semantic compression ratio
        
        # Probability ratios for PPO clipping
        # In GAT mode, distributions output [batch*n_veh, ...] but actions are [batch, ...]
        # We need to handle this shape mismatch
        batch_size = tf.shape(self.a)[0]
        
        if self.use_gat:
            # In GAT mode: distributions are [batch*n_veh, ...], actions are [batch, ...]
            # Solution: Only use the first n_veh outputs (first graph's nodes) for each batch item
            # Or better: reshape and take mean
            
            # For power and rho: distributions output [batch*n_veh, 1] or [batch*n_veh]
            # Actions are [batch]
            # Tile actions to [batch*n_veh] to match distribution shape
            power_action_expanded = tf.tile(tf.expand_dims(power_action, 1), [1, self.n_veh])
            power_action_expanded = tf.reshape(power_action_expanded, [-1])  # [batch*n_veh]
            
            rho_action_expanded = tf.tile(tf.expand_dims(rho_action, 1), [1, self.n_veh])
            rho_action_expanded = tf.reshape(rho_action_expanded, [-1])  # [batch*n_veh]
            
            # Compute probabilities
            # In GAT mode, distributions are [batch*n_veh] and actions are [batch*n_veh]
            # TensorFlow may broadcast, producing [batch*n_veh, batch*n_veh] matrices
            # We need to extract diagonal elements: prob[i] for distribution[i] and action[i]
            
            # Helper function to extract diagonal if matrix, or flatten if vector
            # Use shape size instead of rank to avoid graph construction issues
            def extract_prob(prob_tensor):
                prob_shape = tf.shape(prob_tensor)
                prob_ndims = tf.size(prob_shape)
                return tf.cond(
                    tf.equal(prob_ndims, 2),
                    lambda: tf.linalg.diag_part(tf.reshape(prob_tensor, [batch_size * self.n_veh, batch_size * self.n_veh])),
                    lambda: tf.reshape(prob_tensor, [-1])
                )
            
            # For power: Normal distribution
            power_prob_all = pi.prob(power_action_expanded)
            old_power_prob_all = old_pi.prob(power_action_expanded)
            power_prob_all = extract_prob(power_prob_all)
            old_power_prob_all = extract_prob(old_power_prob_all)
            
            # For rho: Beta distribution
            rho_prob_all = rho_distribution.prob(rho_action_expanded)
            old_rho_prob_all = old_rho_distribution.prob(rho_action_expanded)
            rho_prob_all = extract_prob(rho_prob_all)
            old_rho_prob_all = extract_prob(old_rho_prob_all)
            
            # Reshape to [batch, n_veh] and take mean (aggregate over nodes in same graph)
            power_prob = tf.reduce_mean(tf.reshape(power_prob_all, [batch_size, self.n_veh]), axis=1)
            old_power_prob = tf.reduce_mean(tf.reshape(old_power_prob_all, [batch_size, self.n_veh]), axis=1)
            
            rho_prob = tf.reduce_mean(tf.reshape(rho_prob_all, [batch_size, self.n_veh]), axis=1)
            old_rho_prob = tf.reduce_mean(tf.reshape(old_rho_prob_all, [batch_size, self.n_veh]), axis=1)
        else:
            # MLP mode: shapes should match
            power_prob = pi.prob(power_action)
            old_power_prob = old_pi.prob(power_action)
            if len(power_prob.get_shape()) > 1:
                power_prob = tf.squeeze(power_prob, axis=-1)
            if len(old_power_prob.get_shape()) > 1:
                old_power_prob = tf.squeeze(old_power_prob, axis=-1)
            
            rho_prob = rho_distribution.prob(rho_action)
            old_rho_prob = old_rho_distribution.prob(rho_action)
            if len(rho_prob.get_shape()) > 1:
                rho_prob = tf.squeeze(rho_prob, axis=-1)
            if len(old_rho_prob.get_shape()) > 1:
                old_rho_prob = tf.squeeze(old_rho_prob, axis=-1)
        
        # Add numerical stability: prevent division by zero and handle NaN/Inf
        # Clip probabilities to prevent extreme values
        power_prob = tf.clip_by_value(power_prob, 1e-10, 1e10)
        old_power_prob = tf.clip_by_value(old_power_prob, 1e-10, 1e10)
        rho_prob = tf.clip_by_value(rho_prob, 1e-10, 1e10)
        old_rho_prob = tf.clip_by_value(old_rho_prob, 1e-10, 1e10)
        
        ratio_power = power_prob / (old_power_prob + 1e-8)
        ratio_rho = rho_prob / (old_rho_prob + 1e-8)
        
        # Clip ratios to prevent extreme values that cause NaN
        ratio_power = tf.clip_by_value(ratio_power, 1e-6, 1e6)
        ratio_rho = tf.clip_by_value(ratio_rho, 1e-6, 1e6)

        # Add numerical stability for value function loss
        # Clip values to prevent extreme gradients
        v_target = self.reward + self.gamma * self.v_pred_next  # [batch]
        v_target = tf.clip_by_value(v_target, -1e6, 1e6)
        
        # Handle shape mismatch in GAT mode
        # In GAT mode, self.v is [batch, n_veh], but v_target is [batch]
        # We need to aggregate v_pred to match v_target shape
        if self.use_gat:
            # v_pred is [batch, n_veh], take mean over nodes to get [batch]
            v_pred = tf.reduce_mean(self.v, axis=1)  # [batch]
        else:
            # MLP mode: v_pred is [batch, 1] or [batch]
            v_pred = tf.squeeze(self.v, axis=-1) if len(self.v.get_shape()) > 1 else self.v
        
        v_pred = tf.clip_by_value(v_pred, -1e6, 1e6)
        L_vf = tf.reduce_mean(tf.square(v_target - v_pred))
        
        # Add numerical stability: clip GAE advantage to prevent extreme values
        GAE_advantage_clipped = tf.clip_by_value(GAE_advantage, -1e6, 1e6)
        
        # PPO clipping loss for power action
        L_clip_power = tf.reduce_mean(tf.minimum(
            ratio_power * GAE_advantage_clipped,
            tf.clip_by_value(ratio_power, 1 - epsilon, 1 + epsilon) * GAE_advantage_clipped
        ))
        
        # PPO clipping loss for RB action
        # Handle shape mismatch in GAT mode
        if self.use_gat:
            # In GAT mode: RB_distribution.probs is [batch*n_veh, n_RB], but RB_action is [batch]
            # Tile RB_action to match [batch*n_veh]
            RB_action_expanded = tf.tile(tf.expand_dims(RB_action, 1), [1, self.n_veh])
            RB_action_expanded = tf.reshape(RB_action_expanded, [-1])  # [batch*n_veh]
            
            # Compute probabilities: Categorical.prob() returns [batch*n_veh]
            # But if broadcasting happens, it might be [batch*n_veh, batch*n_veh]
            RB_prob_all = RB_distribution.prob(RB_action_expanded)
            old_RB_prob_all = old_RB_distribution.prob(RB_action_expanded)
            
            # Get shape to determine if it's a matrix
            RB_prob_shape = tf.shape(RB_prob_all)
            RB_prob_ndims = tf.size(RB_prob_shape)
            
            # If it's a matrix (2D), extract diagonal; otherwise keep as is
            RB_prob_all = tf.cond(
                tf.equal(RB_prob_ndims, 2),
                lambda: tf.linalg.diag_part(tf.reshape(RB_prob_all, [batch_size * self.n_veh, batch_size * self.n_veh])),
                lambda: tf.reshape(RB_prob_all, [-1])
            )
            old_RB_prob_shape = tf.shape(old_RB_prob_all)
            old_RB_prob_ndims = tf.size(old_RB_prob_shape)
            old_RB_prob_all = tf.cond(
                tf.equal(old_RB_prob_ndims, 2),
                lambda: tf.linalg.diag_part(tf.reshape(old_RB_prob_all, [batch_size * self.n_veh, batch_size * self.n_veh])),
                lambda: tf.reshape(old_RB_prob_all, [-1])
            )
            
            # Reshape to [batch, n_veh] and take mean
            RB_prob = tf.reduce_mean(tf.reshape(RB_prob_all, [batch_size, self.n_veh]), axis=1)
            old_RB_prob = tf.reduce_mean(tf.reshape(old_RB_prob_all, [batch_size, self.n_veh]), axis=1)
        else:
            # MLP mode: shapes should match
            RB_prob = RB_distribution.prob(RB_action)
            old_RB_prob = old_RB_distribution.prob(RB_action)
        
        # Add numerical stability for RB ratio
        RB_prob = tf.clip_by_value(RB_prob, 1e-10, 1e10)
        old_RB_prob = tf.clip_by_value(old_RB_prob, 1e-10, 1e10)
        ratio_RB = RB_prob / (old_RB_prob + 1e-8)
        ratio_RB = tf.clip_by_value(ratio_RB, 1e-6, 1e6)
        
        L_clip_RB = tf.reduce_mean(tf.minimum(
            ratio_RB * GAE_advantage_clipped,
            tf.clip_by_value(ratio_RB, 1 - epsilon, 1 + epsilon) * GAE_advantage_clipped
        ))
        
        # PPO clipping loss for compression ratio (rho) action
        L_clip_rho = tf.reduce_mean(tf.minimum(
            ratio_rho * GAE_advantage_clipped,
            tf.clip_by_value(ratio_rho, 1 - epsilon, 1 + epsilon) * GAE_advantage_clipped
        ))

        # Entropy for exploration
        S = tf.reduce_mean(pi.entropy() + RB_distribution.entropy() + rho_distribution.entropy())
        
        # Add numerical stability: check for NaN/Inf in loss components
        # Replace NaN/Inf with 0 to prevent training failure
        L_clip_power = tf.where(tf.is_finite(L_clip_power), L_clip_power, tf.zeros_like(L_clip_power))
        L_clip_RB = tf.where(tf.is_finite(L_clip_RB), L_clip_RB, tf.zeros_like(L_clip_RB))
        L_clip_rho = tf.where(tf.is_finite(L_clip_rho), L_clip_rho, tf.zeros_like(L_clip_rho))
        L_vf = tf.where(tf.is_finite(L_vf), L_vf, tf.zeros_like(L_vf))
        S = tf.where(tf.is_finite(S), S, tf.zeros_like(S))

        # Total loss
        L = L_clip_power + L_clip_RB + L_clip_rho - c1 * L_vf + c2 * S
        # Final check: replace NaN/Inf in total loss
        L = tf.where(tf.is_finite(L), L, tf.zeros_like(L))
        
        self.Loss = [L_clip_power, L_clip_RB, L_clip_rho, L_vf, S]
        self.Entropy_value = S
        
        # Sample actions: [RB, Power, Compression Ratio]
        # In GAT mode, output is [n_veh, action_dim] - one action per node
        # In MLP mode, output is [1, action_dim] - single action
        if self.use_gat:
            # GAT mode: sample for all nodes, output [n_veh, action_dim]
            # actor_input is [batch*n_veh, hidden_dim], so distributions output [batch*n_veh, ...]
            # For choose_action, we use batch=1, so [n_veh, ...]
            RB_sample = tf.transpose(tf.cast(RB_distribution.sample(1), dtype=tf.float32))  # [batch*n_veh, 1] -> transpose to [1, batch*n_veh] -> [batch*n_veh, 1] after transpose
            # Actually, RB_distribution.sample(1) returns [1, batch*n_veh], transpose gives [batch*n_veh, 1]
            # But we need [n_veh, 1] for single graph, so reshape
            RB_sample = tf.reshape(RB_sample, [-1, 1])  # Ensure [batch*n_veh, 1] or [n_veh, 1]
            
            power_sample_raw = pi.sample(1)  # [1, batch*n_veh] or [batch*n_veh, 1]
            power_sample = tf.reshape(power_sample_raw, [-1])  # [batch*n_veh] or [n_veh]
            
            rho_sample_raw = rho_distribution.sample(1)  # [1, batch*n_veh] or [batch*n_veh, 1]
            rho_sample = tf.reshape(rho_sample_raw, [-1])  # [batch*n_veh] or [n_veh]
            
            # Ensure all have same first dimension
            # Get the actual shape at runtime
            n_nodes = tf.shape(RB_sample)[0]  # batch*n_veh or n_veh
            
            # Reshape to ensure consistent shapes
            RB_sample = tf.reshape(RB_sample, [n_nodes, 1])  # [n_nodes, 1]
            power_sample = tf.reshape(power_sample, [n_nodes])  # [n_nodes]
            rho_sample = tf.reshape(rho_sample, [n_nodes])  # [n_nodes]
            
            # Concatenate: [n_nodes, 3]
            self.choose_action_op = tf.concat([RB_sample, tf.expand_dims(power_sample, 1), tf.expand_dims(rho_sample, 1)], axis=1)
        else:
            # MLP mode: single action output [1, action_dim]
            RB_sample = tf.transpose(tf.cast(RB_distribution.sample(1), dtype=tf.float32))
            power_sample = tf.squeeze(pi.sample(1), axis=0)
            rho_sample = tf.squeeze(rho_distribution.sample(1), axis=0)
            self.choose_action_op = tf.concat([RB_sample, power_sample, rho_sample], 1)
        # Compute gradients for monitoring
        optimizer = tf.train.AdamOptimizer(lr)
        grads_and_vars = optimizer.compute_gradients(-L)
        
        # Compute global gradient norm for monitoring
        gradients = [grad for grad, var in grads_and_vars if grad is not None]
        if gradients:
            # Clip gradients to prevent exploding gradients
            clipped_gradients, _ = tf.clip_by_global_norm(gradients, clip_norm=0.3)  # Stricter clipping for stability
            self.gradient_norm = tf.global_norm(clipped_gradients)
        else:
            self.gradient_norm = tf.constant(0.0)
        
        # Apply gradients
        self.train_op = optimizer.apply_gradients(grads_and_vars)
        
        # Store action distribution statistics for monitoring
        # rho_distribution is Beta distribution, compute mean and std
        # Beta distribution has mean() and variance() methods
        rho_mean_dist = rho_distribution.mean()  # [batch*n_veh] or [batch]
        rho_var_dist = rho_distribution.variance()  # [batch*n_veh] or [batch]
        self.rho_mean = tf.reduce_mean(rho_mean_dist)
        self.rho_std = tf.sqrt(tf.reduce_mean(rho_var_dist))
        
        self.update_params_op = [tf.assign(r, v) for r, v in zip(old_params, params)]
        self.sesses = []
        for ind_agent in range(self.n_veh):  # initialize agents
            print("Initializing agent", ind_agent)
            sess = tf.Session(config=my_config)
            sess.run(tf.global_variables_initializer())
            self.sesses.append(sess)
        if self.IS_meta:
            print("\nRestoring the model...")
            # 获取优化目标信息，构建模型路径
            optimization_target = args.optimization_target if hasattr(args, 'optimization_target') else 'SE_EE'
            beta = args.beta if hasattr(args, 'beta') else 0.5
            
            opt_target_str = optimization_target.replace('_', '&')  # SE_EE -> SE&EE
            if optimization_target == 'SE_EE':
                opt_suffix = f'{opt_target_str}_{beta:.2f}'  # SE&EE_0.50
            else:
                opt_suffix = opt_target_str  # SE 或 EE
            
            for i in range(self.n_veh):
                # Meta模型保存时使用 meta_model_ 前缀，加载时也需要使用相同的前缀
                meta_save_path = 'meta_model_'  # 强制使用 meta_model_ 前缀
                model_path = meta_save_path + 'AC_' + opt_suffix + '_' + '%s_' %sigma_add + '%d_' % meta_episode +'%s_' %args.lr_meta_a
                self.load_models(self.sesses[i], model_path, self.saver)

    def load_models(self, sess, model_path, saver):
        """ Restore models from the current directory with the name filename """
        dir_ = os.path.dirname(os.path.realpath(__file__))
        full_model_path = os.path.join(dir_, "model/" + model_path)
        
        # Check if model file exists before loading
        if os.path.exists(full_model_path + '.index') or os.path.exists(full_model_path + '.meta'):
            try:
                saver.restore(sess, full_model_path)
                print(f"Model loaded from {full_model_path}")
            except Exception as e:
                print(f"Warning: Could not load model from {full_model_path}: {e}")
                print("Starting with fresh model...")
        else:
            print(f"Model file not found at {full_model_path}, starting with fresh model...")

    def save_model(self, sess, model_path, saver):
        """ Save models to the current directory with the name filename """

        current_dir = os.path.dirname(os.path.realpath(__file__))
        model_path = os.path.join(current_dir, "model/" + model_path)
        if not os.path.exists(os.path.dirname(model_path)):
            os.makedirs(os.path.dirname(model_path))
        saver.save(sess, model_path, write_meta_graph=False)

    def save_models(self, label):
        for i in range(self.n_veh):
            model_path = label + '/agent_' + str(i)
            self.save_model(self.sesses[i], model_path, self.saver)
    def _build_net(self, scope, trainable):
        """
        Build network with GAT encoder and multiple actor heads
        Returns: (power_dist, RB_dist, rho_dist, value, params, saver)
        """
        with tf.variable_scope(scope):
            initializer = tf.compat.v1.keras.initializers.he_normal()
            
            if self.use_gat:
                # Graph Attention Network (GAT) encoder
                # Support batch processing: [batch_size, n_veh, node_feature_dim]
                batch_size = tf.shape(self.node_features)[0]
                
                # Performance optimization: For training efficiency, process only first graph
                # and reuse for batch (graphs in same episode are similar)
                # This significantly speeds up training while maintaining reasonable accuracy
                
                # Get first graph
                node_feat = self.node_features[0]  # [n_veh, node_feature_dim]
                adj = self.adj_matrix[0]  # [n_veh, n_veh]
                
                # Multi-layer GAT encoder (process once)
                gat_output = multi_layer_gat(
                    node_feat, adj,
                    hidden_dims=[n_hidden_1, n_hidden_2, n_hidden_3],
                    num_heads=self.num_gat_heads,
                    activation=tf.nn.relu,
                    dropout_rate=0.0,
                    is_training=trainable,
                    name='gat_encoder'
                )  # [n_veh, n_hidden_3 * num_heads]
                
                # Tile for batch (efficient: same graph structure in batch)
                # This is much faster than processing each graph separately
                gat_output_tiled = tf.tile(tf.expand_dims(gat_output, 0), [batch_size, 1, 1])
                # [batch, n_veh, n_hidden_3 * num_heads]
                
                # Reshape for actor: [batch * n_veh, hidden_dim]
                actor_input = tf.reshape(gat_output_tiled, [-1, gat_output.get_shape()[-1].value])
                
                # For critic: aggregate node embeddings to graph-level
                graph_embeddings = tf.reduce_mean(gat_output_tiled, axis=1)  # [batch, n_hidden_3 * num_heads]
                critic_input = graph_embeddings
                
            else:
                # MLP encoder (backward compatibility)
                self.w_1 = tf.Variable(initializer(shape=(self.s_dim, n_hidden_1)), trainable=trainable)
                self.w_2 = tf.Variable(initializer(shape=(n_hidden_1, n_hidden_2)), trainable=trainable)
                self.w_3 = tf.Variable(initializer(shape=(n_hidden_2, n_hidden_3)), trainable=trainable)
                
                self.b_1 = tf.Variable(tf.truncated_normal([n_hidden_1], stddev=0.1), trainable=trainable)
                self.b_2 = tf.Variable(tf.truncated_normal([n_hidden_2], stddev=0.1), trainable=trainable)
                self.b_3 = tf.Variable(tf.truncated_normal([n_hidden_3], stddev=0.1), trainable=trainable)
                
                layer_p1 = tf.nn.relu(tf.add(tf.matmul(self.s_input, self.w_1), self.b_1), name='p_1')
                layer_1_b = tf.layers.batch_normalization(layer_p1)
                layer_p2 = tf.nn.relu(tf.add(tf.matmul(layer_1_b, self.w_2), self.b_2), name='p_2')
                layer_2_b = tf.layers.batch_normalization(layer_p2)
                layer_p3 = tf.nn.relu(tf.add(tf.matmul(layer_2_b, self.w_3), self.b_3), name='p_3')
                layer_3_b = tf.layers.batch_normalization(layer_p3)
                
                actor_input = layer_2_b  # For actor heads
                critic_input = layer_3_b  # For critic
            
            # Actor Head 1: Power (Continuous, Normal distribution)
            with tf.variable_scope('power_head'):
                if self.use_gat:
                    # Project to hidden dimension for power head
                    power_hidden_dim = n_hidden_2
                    W_power_proj = tf.get_variable('W_power_proj', 
                                                  shape=[actor_input.get_shape()[-1].value, power_hidden_dim],
                                                  initializer=initializer, trainable=trainable)
                    power_hidden = tf.nn.relu(tf.matmul(actor_input, W_power_proj))
                else:
                    power_hidden = actor_input
                
                self.w_mu = tf.get_variable('w_mu', shape=[power_hidden.get_shape()[-1].value, 1],
                                           initializer=initializer, trainable=trainable)
                self.w_sigma = tf.get_variable('w_sigma', shape=[power_hidden.get_shape()[-1].value, 1],
                                              initializer=initializer, trainable=trainable)
                self.b_mu = tf.get_variable('b_mu', shape=[1], 
                                          initializer=tf.constant_initializer(0.0), trainable=trainable)
                self.b_sigma = tf.get_variable('b_sigma', shape=[1],
                                             initializer=tf.constant_initializer(0.0), trainable=trainable)
                
                mu = tf.nn.tanh(tf.add(tf.matmul(power_hidden, self.w_mu), self.b_mu), name='mu_layer')
                sigma = tf.nn.softplus(tf.add(tf.matmul(power_hidden, self.w_sigma), self.b_sigma), name='sigma_layer')
                mu = mu * self.a_bound[1]  # Scale to power bound
                # Add numerical stability: ensure sigma is positive and not too small
                sigma = sigma + sigma_add
                sigma = tf.maximum(sigma, 1e-6)  # Prevent sigma from being too small (causes NaN in Normal distribution)
                sigma = tf.minimum(sigma, 10.0)  # Prevent sigma from being too large (causes instability)
                power_dist = tf.distributions.Normal(loc=mu, scale=sigma)
            
            # Actor Head 2: RB Selection (Discrete, Categorical)
            with tf.variable_scope('rb_head'):
                if self.use_gat:
                    rb_hidden_dim = n_hidden_2
                    W_rb_proj = tf.get_variable('W_rb_proj',
                                               shape=[actor_input.get_shape()[-1].value, rb_hidden_dim],
                                               initializer=initializer, trainable=trainable)
                    rb_hidden = tf.nn.relu(tf.matmul(actor_input, W_rb_proj))
                else:
                    rb_hidden = actor_input
                
                self.w_RB = tf.get_variable('w_RB', shape=[rb_hidden.get_shape()[-1].value, self.n_RB],
                                           initializer=initializer, trainable=trainable)
                # Initialize bias to encourage uniform distribution initially
                # This helps prevent all agents from choosing the same RB
                uniform_bias = tf.constant_initializer(1.0 / self.n_RB)
                self.b_RB = tf.get_variable('b_RB', shape=[self.n_RB],
                                           initializer=uniform_bias, trainable=trainable)
                
                RB_logits = tf.add(tf.matmul(rb_hidden, self.w_RB), self.b_RB, name='RB_logits')
                
                # Add numerical stability: clip logits to prevent extreme values
                RB_logits = tf.clip_by_value(RB_logits, -50.0, 50.0)
                
                # Add temperature scaling to encourage exploration
                # Higher temperature = more uniform distribution (better exploration)
                temperature = 1.2  # Temperature parameter for exploration
                RB_logits_scaled = RB_logits / temperature
                
                RB_probs = tf.nn.softmax(RB_logits_scaled, name='RB_probs')
                
                # Add small epsilon and renormalize to prevent numerical issues
                epsilon_prob = 1e-8
                RB_probs = RB_probs + epsilon_prob
                RB_probs = RB_probs / (tf.reduce_sum(RB_probs, axis=1, keepdims=True) + 1e-10)
                # Ensure probabilities are in valid range
                RB_probs = tf.clip_by_value(RB_probs, 1e-8, 1.0 - 1e-8)
                
                RB_distribution = tf.distributions.Categorical(probs=RB_probs)
            
            # Actor Head 3: Semantic Compression Ratio (rho) [0, 1]
            # Use Beta distribution or sigmoid-normal
            with tf.variable_scope('compression_head'):
                if self.use_gat:
                    rho_hidden_dim = n_hidden_2
                    W_rho_proj = tf.get_variable('W_rho_proj',
                                                shape=[actor_input.get_shape()[-1].value, rho_hidden_dim],
                                                initializer=initializer, trainable=trainable)
                    rho_hidden = tf.nn.relu(tf.matmul(actor_input, W_rho_proj))
                else:
                    rho_hidden = actor_input
                
                # Option 1: Beta distribution (more flexible)
                # Beta distribution parameters: alpha and beta > 0
                self.w_alpha = tf.get_variable('w_alpha', shape=[rho_hidden.get_shape()[-1].value, 1],
                                              initializer=initializer, trainable=trainable)
                self.w_beta = tf.get_variable('w_beta', shape=[rho_hidden.get_shape()[-1].value, 1],
                                             initializer=initializer, trainable=trainable)
                self.b_alpha = tf.get_variable('b_alpha', shape=[1],
                                              initializer=tf.constant_initializer(0.0), trainable=trainable)
                self.b_beta = tf.get_variable('b_beta', shape=[1],
                                             initializer=tf.constant_initializer(0.0), trainable=trainable)
                
                alpha_logit = tf.add(tf.matmul(rho_hidden, self.w_alpha), self.b_alpha)
                beta_logit = tf.add(tf.matmul(rho_hidden, self.w_beta), self.b_beta)
                # Clip logits to prevent extreme values
                alpha_logit = tf.clip_by_value(alpha_logit, -10.0, 10.0)
                beta_logit = tf.clip_by_value(beta_logit, -10.0, 10.0)
                # Ensure alpha, beta > 1 (for Beta distribution to be well-defined)
                alpha = tf.nn.softplus(alpha_logit) + 1.0
                beta = tf.nn.softplus(beta_logit) + 1.0
                # Clip to prevent extreme values that cause NaN
                alpha = tf.clip_by_value(alpha, 1.0, 100.0)
                beta = tf.clip_by_value(beta, 1.0, 100.0)
                rho_distribution = tf.distributions.Beta(concentration1=alpha, concentration0=beta)
            
            # Critic: Value function estimation
            with tf.variable_scope('critic'):
                if self.use_gat:
                    # In GAT mode, critic should output one value per node
                    # critic_input is [batch, n_hidden_3 * num_heads] (graph-level embedding)
                    # But we need per-node values, so we should use actor_input (node-level embeddings)
                    # Actually, for per-node values, we can use gat_output_tiled directly
                    # But for now, let's use graph-level embedding and expand to per-node
                    critic_hidden_dim = n_hidden_3
                    W_critic_proj = tf.get_variable('W_critic_proj',
                                                   shape=[critic_input.get_shape()[-1].value, critic_hidden_dim],
                                                   initializer=initializer, trainable=trainable)
                    critic_hidden = tf.nn.relu(tf.matmul(critic_input, W_critic_proj))  # [batch, critic_hidden_dim]
                    
                    # For per-node values, we can use node-level embeddings from actor
                    # Use gat_output_tiled which is [batch, n_veh, hidden_dim]
                    # Project to per-node values
                    # Actually, let's use a simpler approach: use graph embedding for now
                    # But expand to [batch, n_veh] by tiling
                    self.w_v = tf.get_variable('w_v', shape=[critic_hidden.get_shape()[-1].value, 1],
                                              initializer=initializer, trainable=trainable)
                    self.b_v = tf.get_variable('b_v', shape=[1],
                                             initializer=tf.constant_initializer(0.0), trainable=trainable)
                    
                    # Output graph-level value: [batch, 1]
                    v_graph = tf.nn.relu(tf.add(tf.matmul(critic_hidden, self.w_v), self.b_v), name='v_layer')
                    # Expand to per-node: [batch, n_veh] (each node gets the same graph value for now)
                    # TODO: In future, can use per-node embeddings for better per-node values
                    v = tf.tile(v_graph, [1, self.n_veh])  # [batch, n_veh]
                else:
                    critic_hidden = critic_input
                    self.w_v = tf.get_variable('w_v', shape=[critic_hidden.get_shape()[-1].value, 1],
                                              initializer=initializer, trainable=trainable)
                    self.b_v = tf.get_variable('b_v', shape=[1],
                                             initializer=tf.constant_initializer(0.0), trainable=trainable)
                    v = tf.nn.relu(tf.add(tf.matmul(critic_hidden, self.w_v), self.b_v), name='v_layer')
            
            saver = tf.train.Saver(max_to_keep=self.n_veh * 2)
            
        params = tf.global_variables(scope)
        return power_dist, RB_distribution, rho_distribution, v, params, saver

    def get_v(self, s, sess, node_features=None, adj_matrix=None, agent_idx=0):
        """
        Get value function estimate
        Args:
            s: state (for backward compatibility with MLP)
            sess: TensorFlow session
            node_features: [n_veh, node_feature_dim] node features (for GAT)
            adj_matrix: [n_veh, n_veh] adjacency matrix (for GAT)
            agent_idx: agent index (for GAT mode, to select which node's value)
        """
        if self.use_gat and node_features is not None and adj_matrix is not None:
            # Expand dimensions for batch
            node_features_batch = node_features[np.newaxis, :, :]  # [1, n_veh, node_feature_dim]
            adj_matrix_batch = adj_matrix[np.newaxis, :, :]  # [1, n_veh, n_veh]
            v_all = sess.run(self.v, {
                self.node_features: node_features_batch,
                self.adj_matrix: adj_matrix_batch
            })  # [1, n_veh] in GAT mode (one value per node after fix)
            
            # Extract value for this specific agent (node)
            v_all = np.array(v_all)
            if len(v_all.shape) == 2:
                # [1, n_veh] -> select agent_idx
                if v_all.shape[1] > agent_idx:
                    return float(v_all[0, agent_idx])
                else:
                    return float(v_all[0, 0])
            elif len(v_all.shape) == 1:
                # [n_veh] -> select agent_idx
                if len(v_all) > agent_idx:
                    return float(v_all[agent_idx])
                else:
                    return float(v_all[0])
            else:
                # Fallback: squeeze and take first
                return float(np.squeeze(v_all))
        else:
            # MLP mode (backward compatibility)
            return sess.run(self.v, {
                self.s_input: np.array([s])
            }).squeeze()

    def choose_action(self, s, sess, node_features=None, adj_matrix=None, agent_idx=0):
        """
        Choose action
        Args:
            s: state (for backward compatibility with MLP)
            sess: TensorFlow session
            node_features: [n_veh, node_feature_dim] node features (for GAT)
            adj_matrix: [n_veh, n_veh] adjacency matrix (for GAT)
            agent_idx: agent index (for GAT mode, to select which node's action)
        Returns:
            action: [RB_index, power, compression_ratio]
        """
        if self.use_gat and node_features is not None and adj_matrix is not None:
            # Expand dimensions for batch
            node_features_batch = node_features[np.newaxis, :, :]  # [1, n_veh, node_feature_dim]
            adj_matrix_batch = adj_matrix[np.newaxis, :, :]  # [1, n_veh, n_veh]
            a_all = sess.run(self.choose_action_op, {
                self.node_features: node_features_batch,
                self.adj_matrix: adj_matrix_batch
            })  # [n_veh, 3] in GAT mode (after fix) or [1, n_veh, 3]
            
            # Extract action for this specific agent (node)
            # In GAT mode, each node gets its own action
            a_all = np.array(a_all)
            if len(a_all.shape) == 3:
                # [1, n_veh, 3] -> select agent_idx
                if a_all.shape[1] > agent_idx:
                    a = a_all[0, agent_idx, :]  # [3]
                else:
                    a = a_all[0, 0, :]
            elif len(a_all.shape) == 2:
                # [n_veh, 3] -> select agent_idx
                if a_all.shape[0] > agent_idx:
                    a = a_all[agent_idx, :]  # [3]
                else:
                    a = a_all[0, :]
            else:
                # Fallback: if shape is different, use first element
                a = np.squeeze(a_all)
                if len(a.shape) > 1:
                    a = a[agent_idx] if a.shape[0] > agent_idx else a[0]
        else:
            # MLP mode (backward compatibility)
            a = np.squeeze(sess.run(self.choose_action_op, {self.s_input: s[np.newaxis, :]}))
        
        clipped_a = np.zeros(self.a_dim)
        # Handle scalar or array input - ensure we get scalar values
        # Convert to numpy array first, then extract scalar
        a_array = np.array(a).flatten()
        
        clipped_a[0] = int(np.clip(a_array[0], 0, self.n_RB - 1))  # RB index
        clipped_a[1] = float(np.clip(a_array[1], -self.a_bound[1], self.a_bound[1]))  # Power
        clipped_a[2] = float(np.clip(a_array[2], 0.0, 1.0))  # Compression ratio [0, 1]
        return clipped_a

    def train(self, s, a, gae, reward, v_pred_next, sess, node_features=None, adj_matrix=None):
        """
        Train the network
        Args:
            s: state (for backward compatibility with MLP)
            a: actions [batch_size, 3] (RB, Power, Compression Ratio)
            gae: GAE advantages
            reward: rewards
            v_pred_next: next state values
            sess: TensorFlow session
            node_features: [batch_size, n_veh, node_feature_dim] (for GAT)
            adj_matrix: [batch_size, n_veh, n_veh] (for GAT)
        """
        sess.run(self.update_params_op)
        
        # Prepare feed dict
        if self.use_gat and node_features is not None and adj_matrix is not None:
            feed_dict = {
                self.node_features: node_features,
                self.adj_matrix: adj_matrix,
                self.a: a,
                self.reward: reward,
                self.v_pred_next: v_pred_next,
                self.gae: gae
            }
        else:
            feed_dict = {
                self.s_input: s,
                self.a: a,
                self.reward: reward,
                self.v_pred_next: v_pred_next,
                self.gae: gae
            }
        
        # K epochs
        for i in range(self.K):
            sess.run(self.train_op, feed_dict)
        
        # Get detailed loss components and diagnostics
        loss_components = sess.run(self.Loss, feed_dict)  # [L_clip_power, L_clip_RB, L_clip_rho, L_vf, S]
        entropy_value = sess.run(self.Entropy_value, feed_dict)
        try:
            gradient_norm = sess.run(self.gradient_norm, feed_dict)
        except:
            gradient_norm = 0.0
        try:
            rho_mean = sess.run(self.rho_mean, feed_dict)
            rho_std = sess.run(self.rho_std, feed_dict)
        except:
            rho_mean = 0.0
            rho_std = 0.0
        
        # Compute actor loss (sum of all policy losses) and critic loss
        actor_loss = loss_components[0] + loss_components[1] + loss_components[2]  # L_clip_power + L_clip_RB + L_clip_rho
        critic_loss = loss_components[3]  # L_vf
        total_loss = actor_loss - self.c1 * critic_loss + self.c2 * entropy_value
        
        # Return detailed diagnostics
        # Format: ([L_clip_power, L_clip_RB, L_clip_rho, L_vf, S, actor_loss, critic_loss, total_loss, entropy_value, gradient_norm, rho_mean, rho_std], entropy_value)
        detailed_loss = list(loss_components) + [actor_loss, critic_loss, total_loss, entropy_value, gradient_norm, rho_mean, rho_std]
        return ([detailed_loss], entropy_value)

    def get_gaes(self, rewards, v_preds, v_preds_next):
        """
        GAE
        :param rewards: r(t)
        :param v_preds: v(st)
        :param v_preds_next: v(st+1)
        :return:
        """
        deltas = [r_t + self.gamma * v_next - v for r_t, v_next, v in zip(rewards, v_preds_next, v_preds)]

        # 计算GAE(lambda = 1), 参见 ppo paper eq(11)
        gaes = copy.deepcopy(deltas)

        # 倒序计算GAE
        for t in reversed(range(len(gaes) - 1)):
            gaes[t] = gaes[t] + self.gamma * self.GAE_discount * gaes[t + 1]
        return gaes

    def averaging_model(self, success_rate, semantic_EE_weights=None):
        """
        Average model parameters across all agents using uniform averaging
        Args:
            success_rate: success rate for each agent (for backward compatibility, not used)
            semantic_EE_weights: average semantic EE for each agent (optional, not used for uniform averaging)
        """
        # Use uniform averaging (equal weights for all agents)
        # This is the standard Federated Averaging (FedAvg) approach
        weights = np.ones(self.n_veh) / self.n_veh
        print(f"Federated Averaging: using uniform weights (1/{self.n_veh} for each agent)")
        
        # Log semantic EE for reference (if provided)
        if semantic_EE_weights is not None and len(semantic_EE_weights) == self.n_veh:
            print(f"Semantic EE per agent (for reference): {semantic_EE_weights}")
        
        mu = 0
        sigma = 1e-8
        
        if self.use_gat:
            # GAT mode: aggregate GAT parameters using TensorFlow variable collection
            self._averaging_model_gat(weights)
        else:
            # MLP mode: aggregate MLP parameters (original implementation)
            self._averaging_model_mlp(weights, sigma)
    
    def _averaging_model_mlp(self, weights, sigma):
        """Average MLP parameters using semantic-EE weights"""
        w_1_mean = np.random.normal(0, sigma, [self.s_dim, n_hidden_1])
        w_2_mean = np.random.normal(0, sigma, [n_hidden_1, n_hidden_2])
        w_3_mean = np.random.normal(0, sigma, [n_hidden_2, n_hidden_3])
        w_mu_mean = np.random.normal(0, sigma, [n_hidden_2, 1])
        w_sigma_mean = np.random.normal(0, sigma, [n_hidden_2, 1])
        w_RB_mean = np.random.normal(0, sigma, [n_hidden_2, self.n_RB])
        w_v_mean = np.random.normal(0, sigma, [n_hidden_3, 1])

        b_1_mean = np.random.normal(0, sigma, [n_hidden_1])
        b_2_mean = np.random.normal(0, sigma, [n_hidden_2])
        b_3_mean = np.random.normal(0, sigma, [n_hidden_3])
        b_mu_mean = np.random.normal(0, sigma, [1])
        b_sigma_mean = np.random.normal(0, sigma, [1])
        b_RB_mean = np.random.normal(0, sigma, [self.n_RB])
        b_v_mean = np.random.normal(0, sigma, [1])

        for i in range(self.n_veh):
            w_1_mean += self.sesses[i].run(self.w_1) * weights[i]
            w_2_mean += self.sesses[i].run(self.w_2) * weights[i]
            w_3_mean += self.sesses[i].run(self.w_3) * weights[i]
            w_mu_mean += self.sesses[i].run(self.w_mu) * weights[i]
            w_sigma_mean += self.sesses[i].run(self.w_sigma) * weights[i]
            w_RB_mean += self.sesses[i].run(self.w_RB) * weights[i]
            w_v_mean += self.sesses[i].run(self.w_v) * weights[i]

            b_1_mean += self.sesses[i].run(self.b_1) * weights[i]
            b_2_mean += self.sesses[i].run(self.b_2) * weights[i]
            b_3_mean += self.sesses[i].run(self.b_3) * weights[i]
            b_mu_mean += self.sesses[i].run(self.b_mu) * weights[i]
            b_sigma_mean += self.sesses[i].run(self.b_sigma) * weights[i]
            b_RB_mean += self.sesses[i].run(self.b_RB) * weights[i]
            b_v_mean += self.sesses[i].run(self.b_v) * weights[i]

        for i in range(self.n_veh):
            self.sesses[i].run(self.w_1.assign(w_1_mean))
            self.sesses[i].run(self.w_2.assign(w_2_mean))
            self.sesses[i].run(self.w_3.assign(w_3_mean))
            self.sesses[i].run(self.w_mu.assign(w_mu_mean))
            self.sesses[i].run(self.w_sigma.assign(w_sigma_mean))
            self.sesses[i].run(self.w_RB.assign(w_RB_mean))
            self.sesses[i].run(self.w_v.assign(w_v_mean))

            self.sesses[i].run(self.b_1.assign(b_1_mean))
            self.sesses[i].run(self.b_2.assign(b_2_mean))
            self.sesses[i].run(self.b_3.assign(b_3_mean))
            self.sesses[i].run(self.b_mu.assign(b_mu_mean))
            self.sesses[i].run(self.b_sigma.assign(b_sigma_mean))
            self.sesses[i].run(self.b_RB.assign(b_RB_mean))
            self.sesses[i].run(self.b_v.assign(b_v_mean))
    
    def _averaging_model_gat(self, weights):
        """Average GAT network parameters using semantic-EE weights"""
        # For GAT, we need to get all trainable variables
        # Since GAT uses variable scopes, we need to collect variables from each agent's graph
        
        # Get variable names from the first agent (they should be the same across agents)
        # We'll use a simpler approach: get variables by name pattern
        
        # Collect all variables that need to be averaged
        # GAT variables are in 'network/gat_encoder' scope
        # Actor/Critic variables are in 'network' scope
        
        # Get all trainable variables from first agent
        all_vars_agent0 = tf.trainable_variables()
        
        # Filter to only 'network' scope (exclude 'old_network')
        network_vars = [v for v in all_vars_agent0 
                       if 'network' in v.name and 'old_network' not in v.name]
        
        if len(network_vars) == 0:
            print("Warning: No network variables found for GAT averaging, using MLP fallback")
            # Fallback: try to use MLP variables if they exist
            if hasattr(self, 'w_1'):
                sigma = 1e-8
                self._averaging_model_mlp(weights, sigma)
            return
        
        # Group variables by their base name (without scope prefix)
        var_groups = {}
        for var in network_vars:
            # Extract base name: e.g., 'network/gat_encoder/gat_layer_0/head_0/W_0:0' -> 'W_0'
            parts = var.name.split('/')
            base_name = parts[-1].split(':')[0]  # Remove ':0' suffix
            full_path = '/'.join(parts[1:])  # Remove 'network' prefix
            
            if full_path not in var_groups:
                var_groups[full_path] = []
            var_groups[full_path].append(var)
        
        # Average each variable group
        averaged_values = {}
        for full_path, var_list in var_groups.items():
            if len(var_list) == 0:
                continue
            
            var_template = var_list[0]
            var_shape = var_template.get_shape().as_list()
            
            # Initialize mean
            var_mean = np.zeros(var_shape, dtype=np.float32)
            
            # Weighted average across all agents
            for i in range(self.n_veh):
                try:
                    # Get the corresponding variable from agent i
                    # Variable names should be consistent across agents
                    var_name = var_template.name
                    # Get variable from agent i's graph
                    agent_var = [v for v in tf.trainable_variables() 
                               if v.name == var_name and 'network' in v.name and 'old_network' not in v.name]
                    if len(agent_var) > 0:
                        var_value = self.sesses[i].run(agent_var[0])
                        var_mean += var_value * weights[i]
                except Exception as e:
                    print(f"Warning: Could not get variable {full_path} from agent {i}: {e}")
                    # Use uniform weight as fallback for this agent
                    if i == 0:
                        var_value = self.sesses[i].run(var_template)
                        var_mean = var_value.copy() * weights[i]
                    else:
                        try:
                            var_value = self.sesses[i].run(var_template)
                            var_mean += var_value * (1.0 / self.n_veh)
                        except:
                            pass
            
            averaged_values[full_path] = (var_template, var_mean)
        
        # Assign averaged values back to all agents
        assigned_count = 0
        for full_path, (var_template, var_mean) in averaged_values.items():
            var_name = var_template.name
            for i in range(self.n_veh):
                try:
                    # Find the corresponding variable in agent i's graph
                    agent_var = [v for v in tf.trainable_variables() 
                               if v.name == var_name and 'network' in v.name and 'old_network' not in v.name]
                    if len(agent_var) > 0:
                        self.sesses[i].run(agent_var[0].assign(var_mean))
                        assigned_count += 1
                        break  # Only assign once per variable
                except Exception as e:
                    pass
        
        print(f"Averaged {len(averaged_values)} GAT parameter groups using semantic-EE weights")