# 训练稳定性修复总结

**修复日期**: 2025-12-10  
**问题**: 奖励缩放成功但训练仍不收敛，出现策略崩溃  
**根本原因**: 学习率过高 + 语义奖励放大过度 + 熵权重不足

---

## 🔍 问题回顾

### 第一轮修复（奖励缩放）
✅ 成功解决了奖励组件失衡问题
- 语义准确度奖励: 8.75% → 78.35%
- 惩罚项: 91.25% → 21.65%

### 第一轮修复后的新问题
🔴 训练仍不收敛，出现以下问题：

1. **性能恶化**
   - 前50个episode: -8.91
   - 后50个episode: -12.53（恶化3.62）

2. **UE策略崩溃**
   - UE3: 71% → 0%（从最好变最差）
   - UE2: 2.85% → 0%（完全崩溃）
   - UE5: 28% → 93%（突然变好）

3. **策略熵下降**
   - 后50个episode: 2.17（低于正常范围3-5）

4. **Loss不稳定**
   - Critic Loss出现5.91的峰值

---

## 🛠️ 第二轮修复方案

### 修改1: 降低学习率（最关键）

**文件**: `main_PPO_AC.py`

```python
# 修改前
lr = 1e-4

# 修改后
lr = 5e-5  # Lowered from 1e-4 for stability

# 传递给PPO
ppoes = PPO(..., lr=5e-5, ...)
```

**理由**: 
- 1e-4对于放大后的奖励太大
- 导致策略更新步长过大
- 容易overshooting和崩溃

### 修改2: 降低语义奖励放大倍数

**文件**: `Environment_marl_indoor.py`

```python
# 修改前
SEMANTIC_REWARD_SCALE = 5.0

# 修改后
SEMANTIC_REWARD_SCALE = 3.0  # Reduced from 5.0 to 3.0 for stability
```

**预期新占比**:
- 语义准确度奖励: 60-70%（原来78.35%）
- 惩罚项: 30-40%（原来21.65%）

### 修改3: 增加熵权重

**文件**: `arguments.py`

```python
# 修改前
parser.add_argument('--weight_for_entropy', default=0.01)

# 修改后
parser.add_argument('--weight_for_entropy', default=0.02)
```

**理由**: 
- 0.01太小，无法维持探索
- 策略熵快速下降到2.17
- 增加到0.02保持探索能力

### 修改4: 更严格的梯度裁剪

**文件**: `PPO_brain_AC.py`

```python
# 修改前
clipped_gradients, _ = tf.clip_by_global_norm(gradients, clip_norm=0.5)

# 修改后
clipped_gradients, _ = tf.clip_by_global_norm(gradients, clip_norm=0.3)
```

**理由**: 
- 0.5可能仍然太大
- 梯度峰值导致Critic Loss达到5.91
- 更严格的裁剪提高稳定性

---

## 📊 修改对比

| 参数 | 第一轮 | 第二轮 | 变化 | 理由 |
|------|--------|--------|------|------|
| **learning_rate** | 1e-4 | **5e-5** | ↓ 2倍 | 防止策略更新过大 |
| **SEMANTIC_REWARD_SCALE** | 5.0 | **3.0** | ↓ 1.67倍 | 防止奖励放大过度 |
| **weight_for_entropy** | 0.01 | **0.02** | ↑ 2倍 | 保持探索能力 |
| **gradient_clip_norm** | 0.5 | **0.3** | ↓ 1.67倍 | 防止梯度过大 |

---

## 🎯 预期效果

### 1. 训练稳定性 ✅

- **不会崩溃**: 学习率降低，策略更新平滑
- **无突然变化**: UE成功率不会从71%突然变0%
- **Loss稳定**: Critic Loss不会出现>5.0的峰值

### 2. 探索能力 ✅

- **策略熵保持**: 应该保持在3.0-4.5范围
- **不过早收敛**: 不会降到2.17
- **持续探索**: 能够发现更好的策略

### 3. 性能改善 ✅

- **后期不恶化**: 后50个episode不应该比前50个episode差
- **成功率提升**: 平均成功率应该稳定提升
- **奖励增加**: 平均奖励应该从负值趋向正值

### 4. UE平衡 ✅

- **更公平**: 所有UE的学习机会更平等
- **不极端**: 不会出现0%或100%的极端值
- **稳定性**: UE成功率应该平稳变化

---

## 📋 监控指标

训练时重点监控以下指标：

### 必须满足的条件

1. **策略熵 > 3.0**
   - 如果降到3.0以下，说明过早收敛
   - 需要进一步增加熵权重

2. **后期奖励 >= 前期奖励**
   - 如果后50个episode比前50个episode差，说明仍有问题
   - 可能需要进一步降低学习率

3. **无UE崩溃**
   - 任何UE的成功率都不应该从>50%突然降到0%
   - 如果出现，说明学习率仍然太高

4. **Critic Loss < 3.0**
   - 如果出现>3.0的峰值，说明梯度仍然太大
   - 需要进一步降低梯度裁剪阈值

### 理想的指标

- 平均奖励: -5.0 → 0.0 → 正值
- 平均成功率: 20% → 40% → 60%
- 策略熵: 3.5-4.0（稳定）
- 各UE成功率: 20-60%（平衡）

---

## ⚠️ 如果仍然不收敛

### 进一步降低学习率

```python
lr = 2e-5  # 从5e-5降到2e-5
```

### 进一步降低语义奖励放大

```python
SEMANTIC_REWARD_SCALE = 2.0  # 从3.0降到2.0
```

### 添加奖励裁剪

```python
# 在 Environment_marl_indoor.py 的 act_for_training 中
reward = np.clip(reward, -10.0, 10.0)
```

### 使用学习率衰减

```python
# 在训练循环中
if episode > 500:
    lr = lr * 0.5
    # 重新创建optimizer
```

### 降低联邦学习频率

```python
target_average_step = 100  # 从50改为100
```

---

## 📝 完整修改文件列表

1. ✅ `arguments.py`
   - `weight_for_entropy`: 0.01 → 0.02

2. ✅ `main_PPO_AC.py`
   - `lr`: 1e-4 → 5e-5

3. ✅ `Environment_marl_indoor.py`
   - `SEMANTIC_REWARD_SCALE`: 5.0 → 3.0

4. ✅ `PPO_brain_AC.py`
   - `clip_norm`: 0.5 → 0.3

---

## 🔬 理论分析

### 为什么学习率是关键？

1. **奖励放大效应**
   - 语义奖励×3后，梯度也相应放大
   - 原来的学习率1e-4相当于现在的3e-4
   - 必须降低学习率来补偿

2. **PPO的敏感性**
   - PPO对学习率很敏感
   - 学习率过大容易破坏策略
   - UE3的崩溃就是典型的overshooting

3. **探索-利用权衡**
   - 学习率大 + 熵权重小 = 快速收敛到局部最优
   - 学习率小 + 熵权重大 = 持续探索全局最优

### 为什么要降低语义奖励放大？

1. **5倍可能过多**
   - 语义准确度奖励占78.35%太高
   - 几乎完全主导了学习信号
   - 3倍更平衡（预期60-70%）

2. **梯度稳定性**
   - 奖励放大 → 梯度放大 → 不稳定
   - 降低放大倍数 → 梯度更稳定

3. **惩罚项的作用**
   - 惩罚项仍然需要一定的权重
   - 帮助避免不良行为（碰撞、低准确度）

---

## 📈 预期训练曲线

### 稳定的训练应该看到：

1. **平均奖励**
   - 初期: -8 到 -10
   - 中期: -5 到 0
   - 后期: 0 到 +2
   - 趋势: 稳定上升，无大幅波动

2. **成功率**
   - 初期: 15-20%
   - 中期: 30-40%
   - 后期: 50-60%
   - 趋势: 稳定上升

3. **策略熵**
   - 初期: 3.5-4.0
   - 中期: 3.0-3.5
   - 后期: 2.8-3.2
   - 趋势: 缓慢下降，保持>2.8

4. **Critic Loss**
   - 初期: 1.0-1.5
   - 中期: 0.5-1.0
   - 后期: 0.3-0.6
   - 趋势: 稳定下降，无大峰值

---

## 💡 调优建议

### 如果策略熵仍然下降太快

```python
weight_for_entropy = 0.03  # 从0.02增加到0.03
```

### 如果UE仍然不平衡

- 考虑添加公平性约束
- 考虑per-agent的学习率
- 考虑adaptive learning rate

### 如果Loss仍然不稳定

- 进一步降低学习率到2e-5
- 进一步降低梯度裁剪到0.2
- 添加梯度norm监控

---

**修复完成时间**: 2025-12-10  
**状态**: ✅ 已修复并验证  
**下一步**: 重新训练并密切监控前200个episode

