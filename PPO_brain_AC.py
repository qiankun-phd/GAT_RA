import numpy as np
import os
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import random
import copy

from arguments import get_args
args = get_args()

os.environ['PYTHONHASHSEED'] = str(args.seed)
random.seed(args.seed)
np.random.seed(args.seed)
tf.set_random_seed(args.set_random_seed)


my_config = tf.ConfigProto()
my_config.gpu_options.allow_growth = True

n_hidden_1 = args.n_hidden_1
n_hidden_2 = args.n_hidden_2
n_hidden_3 = args.n_hidden_3

sigma_add = args.sigma_add

# æ³¨æ„ï¼šæœ¬ç‰ˆæœ¬å·²ç§»é™¤ GAT ç¼–ç å™¨å®ç°ï¼ˆ`graph_attention_layer` / `multi_layer_gat`ï¼‰ã€‚
# å¦‚éœ€æ¢å¤ GATï¼Œè¯·å›é€€åˆ°åŒ…å« GAT çš„ç‰ˆæœ¬æˆ–é‡æ–°å¼•å…¥ç›¸å…³å®ç°ã€‚

class PPO(object):
    def __init__(self, s_dim, a_bound, c1, c2, epsilon, lr, meta_lr, K, n_veh, n_RB, IS_meta, meta_episode,
                 use_gat=False, num_gat_heads=4, node_feature_dim=None):
        """
        Args:
            s_dim: state dimension (for backward compatibility, if use_gat=False)
            a_bound: action bounds [RB_bound, power_bound, compression_bound]
            c1, c2, epsilon, lr, meta_lr, K: PPO hyperparameters
            n_veh: number of UAVs (nodes in graph)
            n_RB: number of resource blocks
            IS_meta: whether to use meta learning
            meta_episode: meta episode number
            use_gat: whether to use GAT instead of MLP
            num_gat_heads: number of attention heads in GAT
            node_feature_dim: dimension of node features (CSI, location, etc.)
        """
        self.a_bound = a_bound
        self.K = K
        self.s_dim = s_dim  # Keep for backward compatibility
        self.a_dim = 3  # RB_choice + Power + Compression Ratio (rho)
        self.n_RB = n_RB
        self.n_veh = n_veh
        self.IS_meta = IS_meta
        self.meta_episode = meta_episode  # Save meta_episode as instance variable
        self.gamma = args.gamma
        self.GAE_discount = args.lambda_advantage
        if use_gat:
            raise ValueError("å½“å‰ç‰ˆæœ¬å·²ç§»é™¤ GAT ç¼–ç å™¨ï¼ˆuse_gat=True ä¸å†æ”¯æŒï¼‰ã€‚")
        self.use_gat = False
        self.c1 = c1  # Value function loss weight
        self.c2 = c2  # Entropy weight
        self.epsilon = epsilon  # PPO clipping parameter
        
        # MLP-based input
        self.s_input = tf.placeholder(tf.float32, shape=(None, self.s_dim), name='s_t')

        power_dist, RB_distribution, rho_distribution, self.v, params, self.saver = self._build_net('network', True)
        old_power_dist, old_RB_distribution, old_rho_distribution, old_v, old_params, _ = self._build_net('old_network', False)
        self.reward = tf.placeholder(tf.float32, shape=[None], name='reward')
        self.v_pred_next = tf.placeholder(dtype=tf.float32, shape=[None], name='v_pred_next')
        self.gae = tf.placeholder(dtype=tf.float32, shape=[None], name='gae')

        GAE_advantage = self.gae

        self.a = tf.placeholder(tf.float32, shape=(None, self.a_dim), name='a_t')
        RB_action = self.a[:,0]
        power_action = self.a[:,1]
        rho_action = self.a[:,2]  # Compression ratio
        
        # Betaåˆ†å¸ƒå¤„ç†ï¼špowerå’Œrhoéƒ½ä½¿ç”¨Betaåˆ†å¸ƒï¼Œè¾“å‡º[0,1]
        # Poweréœ€è¦ä»[-bound, bound]æ˜ å°„åˆ°[0,1]æ¥è®¡ç®—æ¦‚ç‡
        # æ˜ å°„å…¬å¼ï¼špower_normalized = (power_action + action_bound[1]) / (2 * action_bound[1])
        power_normalized = (power_action + self.a_bound[1]) / (2 * self.a_bound[1] + 1e-8)
        power_normalized = tf.clip_by_value(power_normalized, 1e-6, 1.0 - 1e-6)  # é¿å…è¾¹ç•Œå€¼
        power_prob = power_dist.prob(power_normalized)
        old_power_prob = old_power_dist.prob(power_normalized)
        
        # Rhoå·²ç»æ˜¯[0,1]èŒƒå›´ï¼Œç›´æ¥ä½¿ç”¨Betaåˆ†å¸ƒ
        rho_action_clipped = tf.clip_by_value(rho_action, 1e-6, 1.0 - 1e-6)  # é¿å…è¾¹ç•Œå€¼
        rho_prob = rho_distribution.prob(rho_action_clipped)
        old_rho_prob = old_rho_distribution.prob(rho_action_clipped)
        
        # Original code style: simple ratio calculation with numerical stability
        # Add small epsilon to prevent division by zero
        ratio_power = power_prob / (old_power_prob + 1e-8)
        ratio_rho = rho_prob / (old_rho_prob + 1e-8)
        # Clip ratios to prevent extreme values
        ratio_power = tf.clip_by_value(ratio_power, 1e-6, 1e6)
        ratio_rho = tf.clip_by_value(ratio_rho, 1e-6, 1e6)
        
        # Value function loss
        v_pred = tf.squeeze(self.v, axis=-1) if len(self.v.get_shape()) > 1 else self.v
        
        # Clip values to prevent extreme values
        v_target = self.reward + self.gamma * self.v_pred_next
        v_target = tf.clip_by_value(v_target, -10.0, 10.0)
        v_pred = tf.clip_by_value(v_pred, -10.0, 10.0)
        L_vf = tf.reduce_mean(tf.square(v_target - v_pred))
        # Replace NaN with zeros
        L_vf = tf.where(tf.is_finite(L_vf), L_vf, tf.zeros_like(L_vf))
        
        # Use GAE_advantage directly (original code style)
        GAE_advantage_clipped = GAE_advantage
        
        # PPO clipping loss for power action
        L_clip_power = tf.reduce_mean(tf.minimum(
            ratio_power * GAE_advantage_clipped,
            tf.clip_by_value(ratio_power, 1 - epsilon, 1 + epsilon) * GAE_advantage_clipped
        ))
        # Replace NaN with zeros
        L_clip_power = tf.where(tf.is_finite(L_clip_power), L_clip_power, tf.zeros_like(L_clip_power))
        
        # PPO clipping loss for RB action
        RB_prob = RB_distribution.prob(RB_action)
        old_RB_prob = old_RB_distribution.prob(RB_action)
        
        # Original code style: simple ratio calculation with numerical stability
        ratio_RB = RB_prob / (old_RB_prob + 1e-8)
        # Clip ratio to prevent extreme values
        ratio_RB = tf.clip_by_value(ratio_RB, 1e-6, 1e6)
        
        L_RB = tf.reduce_mean(tf.minimum(
            ratio_RB * GAE_advantage_clipped,
            tf.clip_by_value(ratio_RB, 1 - epsilon, 1 + epsilon) * GAE_advantage_clipped
        ))
        # Replace NaN with zeros
        L_RB = tf.where(tf.is_finite(L_RB), L_RB, tf.zeros_like(L_RB))
        
        # PPO clipping loss for compression ratio (rho) action
        L_rho = tf.reduce_mean(tf.minimum(
            ratio_rho * GAE_advantage_clipped,
            tf.clip_by_value(ratio_rho, 1 - epsilon, 1 + epsilon) * GAE_advantage_clipped
        ))
        # Replace NaN with zeros
        L_rho = tf.where(tf.is_finite(L_rho), L_rho, tf.zeros_like(L_rho))
        
        # Entropy for exploration
        S = tf.reduce_mean(power_dist.entropy() + RB_distribution.entropy() + rho_distribution.entropy())
        
        # Total loss (original code style, but with rho added)
        L = L_clip_power + L_RB + L_rho - c1 * L_vf + c2 * S
        # Replace NaN with zeros
        L = tf.where(tf.is_finite(L), L, tf.zeros_like(L))
        
        self.Loss = [L_clip_power, L_RB, L_rho, L_vf, S]
        self.Entropy_value = S
        
        # Sample actions (MLP mode)
        # Betaåˆ†å¸ƒè¾“å‡º[0,1]ï¼Œpoweréœ€è¦æ˜ å°„åˆ°[-bound, bound]
        power_sample_beta = tf.squeeze(power_dist.sample(1), axis=0)  # [0,1]
        # æ˜ å°„åˆ°[-bound, bound]ï¼špower_scaled = power_sample * 2 * bound - bound
        power_sample_scaled = power_sample_beta * 2 * self.a_bound[1] - self.a_bound[1]
        
        rho_sample = tf.squeeze(rho_distribution.sample(1), axis=0)  # [0,1]ï¼ŒBetaåˆ†å¸ƒå¤©ç„¶æœ‰ç•Œ
        
        self.choose_action_op = tf.concat([
            tf.transpose(tf.cast(RB_distribution.sample(1), dtype=tf.float32)),
            power_sample_scaled,
            rho_sample
        ], 1)

        # Optimizer (original code style)
        self.train_op = tf.train.AdamOptimizer(lr).minimize(-L)
        self.update_params_op = [tf.assign(r, v) for r, v in zip(old_params, params)]
        self.sesses = []
        for ind_agent in range(self.n_veh):
            print("Initializing agent", ind_agent)
            sess = tf.Session(config=my_config)
            sess.run(tf.global_variables_initializer())
            self.sesses.append(sess)
        if self.IS_meta:
            print("\nRestoring the model...")
            # ä½¿ç”¨SEEä¼˜åŒ–ç›®æ ‡ï¼ˆä¸meta_train_PPO_AC.pyä¿æŒä¸€è‡´ï¼‰
            optimization_target = 'SEE'
            
            # Metaè®­ç»ƒä¿å­˜æ ¼å¼: AC_SEE_{sigma_add}_{meta_episode}_{lr_meta_a}
            # ä¸backupç‰ˆæœ¬çš„åŒºåˆ«ï¼šbackupç‰ˆæœ¬æ ¼å¼ä¸º AC_{sigma_add}_{meta_episode}_{lr_meta_a}ï¼ˆæ— SEEï¼‰
            # å½“å‰ç‰ˆæœ¬æ ¼å¼ä¸º AC_SEE_{sigma_add}_{meta_episode}_{lr_meta_a}ï¼ˆæœ‰SEEï¼‰
            opt_suffix = optimization_target  # ç›´æ¥ä½¿ç”¨'SEE'
            
            # ä½¿ç”¨å®ä¾‹å˜é‡self.meta_episodeå’Œargsä¸­çš„å‚æ•°
            for i in range(self.n_veh):
                meta_save_path = args.save_path  # ä½¿ç”¨args.save_pathï¼Œé»˜è®¤æ˜¯'meta_model_'
                model_path = meta_save_path + 'AC_' + opt_suffix + '_' + '%s_' %sigma_add + '%d_' % self.meta_episode +'%s_' %args.lr_meta_a
                print(f"Loading meta model for agent {i}: {model_path}")
                print(f"  Expected path format: {model_path}")
                self.load_models(self.sesses[i], model_path, self.saver)

    def load_models(self, sess, model_path, saver):
        """ Restore models from the current directory with the name filename """
        dir_ = os.path.dirname(os.path.realpath(__file__))
        full_model_path = os.path.join(dir_, "model/" + model_path)
        
        # Check if model file exists
        if os.path.exists(full_model_path + '.index'):
            try:
                saver.restore(sess, full_model_path)
                print(f"âœ… Successfully loaded model: {full_model_path}")
            except Exception as e:
                error_str = str(e)
                # æ£€æŸ¥æ˜¯å¦æ˜¯å½¢çŠ¶ä¸åŒ¹é…é”™è¯¯ï¼ˆæ—§æ¨¡å‹æ²¡æœ‰rhoå‚æ•°ï¼‰
                if "shape mismatch" in error_str.lower() or "Assign requires shapes" in error_str:
                    print(f"âš ï¸  Shape mismatch detected!")
                    print(f"   The saved meta model was trained with OLD code (without rho/compression ratio parameters).")
                    print(f"   Current code requires rho parameters, so the model cannot be loaded.")
                    print(f"")
                    print(f"   SOLUTION: Re-train the meta model with current code:")
                    print(f"   python meta_train_PPO_AC.py --n_veh_list 2,4,8 --n_RB 10 --sigma_add 0.3 --meta_episode 100 --lr_meta_a 5e-7 --lr_meta_c 1e-5")
                    print(f"")
                    print(f"   Continuing with randomly initialized weights (no meta model loaded)...")
                else:
                    print(f"âŒ Failed to load model: {full_model_path}")
                    print(f"   Error: {e}")
                    print(f"   Please re-train the meta model using current code.")
                    print(f"   Continuing with randomly initialized weights...")
        else:
            print(f"âš ï¸  Warning: Model file does not exist: {full_model_path}")
            print(f"   Looking for: {full_model_path}.index")
            
            # å°è¯•å…¼å®¹backupç‰ˆæœ¬çš„è·¯å¾„æ ¼å¼ï¼ˆæ— SEEåç¼€ï¼‰
            if 'AC_SEE_' in model_path:
                backup_model_path = model_path.replace('AC_SEE_', 'AC_')
                backup_full_path = os.path.join(dir_, "model/" + backup_model_path)
                if os.path.exists(backup_full_path + '.index'):
                    try:
                        saver.restore(sess, backup_full_path)
                        print(f"âœ… Successfully loaded model (backup format): {backup_full_path}")
                        return
                    except Exception as e:
                        print(f"   Also tried backup format but failed: {backup_full_path}")
            
            print(f"   Please train the meta model first:")
            print(f"   python meta_train_PPO_AC.py --n_veh_list 2,4,8 --n_RB 10 --sigma_add 0.3 --meta_episode 100 --lr_meta_a 5e-7 --lr_meta_c 1e-5")
            print(f"   Continuing with randomly initialized weights...")

    def save_model(self, sess, model_path, saver):
        """ Save models to the current directory with the name filename """
        current_dir = os.path.dirname(os.path.realpath(__file__))
        model_path = os.path.join(current_dir, "model/" + model_path)
        if not os.path.exists(os.path.dirname(model_path)):
            os.makedirs(os.path.dirname(model_path))
        saver.save(sess, model_path, write_meta_graph=False)

    def save_models(self, label):
        for i in range(self.n_veh):
            model_path = label + '/agent_' + str(i)
            self.save_model(self.sesses[i], model_path, self.saver)
            
    def _build_net(self, scope, trainable):
        """
        Build network (MLP encoder) with multiple actor heads
        Returns: (power_dist, RB_dist, rho_dist, value, params, saver)
        """
        with tf.variable_scope(scope):
            initializer = tf.compat.v1.keras.initializers.he_normal()

            # MLP encoder
            # æ³¨æ„ï¼šå˜é‡åˆ›å»ºé¡ºåºå¿…é¡»ä¸meta_brain_PPO.pyå®Œå…¨ä¸€è‡´ï¼Œå¦åˆ™æ— æ³•åŠ è½½metaæ¨¡å‹
            # é¡ºåºï¼šæ‰€æœ‰weightså…ˆåˆ›å»ºï¼Œç„¶åæ‰€æœ‰biases
            self.w_1 = tf.Variable(initializer(shape=(self.s_dim, n_hidden_1)), trainable=trainable)
            self.w_2 = tf.Variable(initializer(shape=(n_hidden_1, n_hidden_2)), trainable=trainable)
            self.w_3 = tf.Variable(initializer(shape=(n_hidden_2, n_hidden_3)), trainable=trainable)
            # Power Betaåˆ†å¸ƒå‚æ•°ï¼šalphaå’Œbeta
            self.w_power_alpha = tf.Variable(initializer(shape=(n_hidden_2, 1)), trainable=trainable)
            self.w_power_beta = tf.Variable(initializer(shape=(n_hidden_2, 1)), trainable=trainable)
            self.w_RB = tf.Variable(initializer(shape=(n_hidden_2, self.n_RB)), trainable=trainable)
            # Rho Betaåˆ†å¸ƒå‚æ•°ï¼šalphaå’Œbeta
            self.w_rho_alpha = tf.Variable(initializer(shape=(n_hidden_2, 1)), trainable=trainable)
            self.w_rho_beta = tf.Variable(initializer(shape=(n_hidden_2, 1)), trainable=trainable)
            self.w_v = tf.Variable(initializer(shape=(n_hidden_3, 1)), trainable=trainable)

            self.b_1 = tf.Variable(tf.truncated_normal([n_hidden_1], stddev=0.1), trainable=trainable)
            self.b_2 = tf.Variable(tf.truncated_normal([n_hidden_2], stddev=0.1), trainable=trainable)
            self.b_3 = tf.Variable(tf.truncated_normal([n_hidden_3], stddev=0.1), trainable=trainable)
            # Power Betaåˆ†å¸ƒbias
            self.b_power_alpha = tf.Variable(tf.truncated_normal([1], stddev=0.1), trainable=trainable)
            self.b_power_beta = tf.Variable(tf.truncated_normal([1], stddev=0.1), trainable=trainable)
            self.b_RB = tf.Variable(tf.truncated_normal([self.n_RB], stddev=0.1), trainable=trainable)
            # Rho Betaåˆ†å¸ƒbias
            self.b_rho_alpha = tf.Variable(tf.truncated_normal([1], stddev=0.1), trainable=trainable)
            self.b_rho_beta = tf.Variable(tf.truncated_normal([1], stddev=0.1), trainable=trainable)
            self.b_v = tf.Variable(tf.truncated_normal([1], stddev=0.1), trainable=trainable)

            layer_p1 = tf.nn.relu(tf.add(tf.matmul(self.s_input, self.w_1), self.b_1), name='p_1')
            layer_1_b = tf.layers.batch_normalization(layer_p1)
            layer_p2 = tf.nn.relu(tf.add(tf.matmul(layer_1_b, self.w_2), self.b_2), name='p_2')
            layer_2_b = tf.layers.batch_normalization(layer_p2)
            layer_p3 = tf.nn.relu(tf.add(tf.matmul(layer_2_b, self.w_3), self.b_3), name='p_3')
            layer_3_b = tf.layers.batch_normalization(layer_p3)

            critic_input = layer_3_b  # For critic
            
            # Actor heads
            RB_probs = tf.nn.softmax(tf.add(tf.matmul(layer_2_b, self.w_RB), self.b_RB), name='RB_layer')
            RB_distribution = tf.distributions.Categorical(probs=RB_probs)
            
            # Power Betaåˆ†å¸ƒï¼šalphaå’Œbetaå‚æ•°ï¼ˆç¡®ä¿>1ä»¥é¿å…æç«¯å€¼ï¼‰
            power_alpha_raw = tf.nn.softplus(tf.add(tf.matmul(layer_2_b, self.w_power_alpha), self.b_power_alpha), name='power_alpha_layer')
            power_beta_raw = tf.nn.softplus(tf.add(tf.matmul(layer_2_b, self.w_power_beta), self.b_power_beta), name='power_beta_layer')
            power_alpha = power_alpha_raw + 1.0  # ç¡®ä¿alpha > 1
            power_beta = power_beta_raw + 1.0    # ç¡®ä¿beta > 1
            power_distribution = tf.distributions.Beta(concentration1=power_alpha, concentration0=power_beta)
            
            # Rho Betaåˆ†å¸ƒï¼šalphaå’Œbetaå‚æ•°ï¼ˆç¡®ä¿>1ä»¥é¿å…æç«¯å€¼ï¼‰
            rho_alpha_raw = tf.nn.softplus(tf.add(tf.matmul(layer_2_b, self.w_rho_alpha), self.b_rho_alpha), name='rho_alpha_layer')
            rho_beta_raw = tf.nn.softplus(tf.add(tf.matmul(layer_2_b, self.w_rho_beta), self.b_rho_beta), name='rho_beta_layer')
            rho_alpha = rho_alpha_raw + 1.0  # ç¡®ä¿alpha > 1
            rho_beta = rho_beta_raw + 1.0    # ç¡®ä¿beta > 1
            rho_distribution = tf.distributions.Beta(concentration1=rho_alpha, concentration0=rho_beta)
            
            # Critic network (MLP)
            v = tf.nn.relu(tf.add(tf.matmul(critic_input, self.w_v), self.b_v), name='v_layer')
            
            saver = tf.train.Saver(max_to_keep=self.n_veh * 2)
            
        params = tf.global_variables(scope)
        return power_distribution, RB_distribution, rho_distribution, v, params, saver

    def get_v(self, s, sess, node_features=None, adj_matrix=None, agent_idx=0):
        """
        Get value function estimate
        Args:
            s: state (for backward compatibility with MLP)
            sess: TensorFlow session
            node_features: [n_veh, node_feature_dim] node features (for GAT)
            adj_matrix: [n_veh, n_veh] adjacency matrix (for GAT)
            agent_idx: agent index (for GAT mode, to select which node's value)
        """
        if node_features is not None or adj_matrix is not None:
            raise ValueError("å½“å‰ç‰ˆæœ¬å·²ç§»é™¤ GAT ç¼–ç å™¨ï¼šget_v ä¸å†æ”¯æŒ node_features/adj_matrix è¾“å…¥ã€‚")
        return sess.run(self.v, {self.s_input: np.array([s])}).squeeze()

    def choose_action(self, s, sess, node_features=None, adj_matrix=None, agent_idx=0):
        """
        Choose action
        Args:
            s: state (for backward compatibility with MLP)
            sess: TensorFlow session
            node_features: [n_veh, node_feature_dim] node features (for GAT)
            adj_matrix: [n_veh, n_veh] adjacency matrix (for GAT)
            agent_idx: agent index (for GAT mode, to select which node's action)
        """
        if node_features is not None or adj_matrix is not None:
            raise ValueError("å½“å‰ç‰ˆæœ¬å·²ç§»é™¤ GAT ç¼–ç å™¨ï¼šchoose_action ä¸å†æ”¯æŒ node_features/adj_matrix è¾“å…¥ã€‚")
        a = np.squeeze(sess.run(self.choose_action_op, {self.s_input: s[np.newaxis, :]}))
        
        clipped_a = np.zeros(self.a_dim)
        clipped_a[0] = a[0]  # RB (Categorical)
        # Powerå·²ç»æ˜¯Betaåˆ†å¸ƒæ˜ å°„åçš„å€¼ï¼Œåœ¨[-bound, bound]èŒƒå›´å†…ï¼Œä½†ä¸ºäº†å®‰å…¨è¿˜æ˜¯clipä¸€ä¸‹
        clipped_a[1] = np.clip(a[1], -self.a_bound[1], self.a_bound[1])
        # Rhoæ˜¯Betaåˆ†å¸ƒè¾“å‡ºï¼Œå¤©ç„¶åœ¨[0,1]èŒƒå›´å†…ï¼Œä½†ä¸ºäº†å®‰å…¨è¿˜æ˜¯clipä¸€ä¸‹
        clipped_a[2] = np.clip(a[2], 0.0, 1.0)
        return clipped_a

    def train(self, s, a, gae, reward, v_pred_next, sess, node_features=None, adj_matrix=None, agent_idx=0):
        """
        Train the network
        Args:
            s: state (for backward compatibility with MLP)
            a: actions [batch_size, 3] (RB, Power, Compression Ratio)
            gae: GAE advantages
            reward: rewards
            v_pred_next: next state values
            sess: TensorFlow session
            node_features: [batch_size, n_veh, node_feature_dim] (for GAT)
            adj_matrix: [batch_size, n_veh, n_veh] (for GAT)
        """
        sess.run(self.update_params_op)
        
        if node_features is not None or adj_matrix is not None:
            raise ValueError("å½“å‰ç‰ˆæœ¬å·²ç§»é™¤ GAT ç¼–ç å™¨ï¼štrain ä¸å†æ”¯æŒ node_features/adj_matrix è¾“å…¥ã€‚")
        feed_dict = {
            self.s_input: s,
            self.a: a,
            self.reward: reward,
            self.v_pred_next: v_pred_next,
            self.gae: gae
        }
        
        # K epochs
        for i in range(self.K):
            sess.run(self.train_op, feed_dict)
        
        # Get loss components for debugging
        loss_components = sess.run(self.Loss, feed_dict)
        entropy = sess.run(self.Entropy_value, feed_dict)
        
        return [loss_components, entropy]

    def get_gaes(self, rewards, v_preds, v_preds_next):
        """
        GAE
        :param rewards: r(t) - can be [T, n_veh] or [T]
        :param v_preds: v(st) - can be [T, n_veh] or [T]
        :param v_preds_next: v(st+1) - can be [T, n_veh] or [T]
        :return: gaes - same shape as input
        """
        # Convert to numpy arrays if needed
        rewards = np.array(rewards)
        v_preds = np.array(v_preds)
        v_preds_next = np.array(v_preds_next)
        
        # Handle 2D case: [T, n_veh]
        if len(rewards.shape) == 2:
            # For each agent, compute GAE separately
            n_veh = rewards.shape[1]
            gaes_list = []
            for agent_idx in range(n_veh):
                r_agent = rewards[:, agent_idx]
                v_agent = v_preds[:, agent_idx]
                v_next_agent = v_preds_next[:, agent_idx]
                
                # Compute deltas for this agent
                deltas = r_agent + self.gamma * v_next_agent - v_agent
                gaes = deltas.copy()
                
                # Compute GAE backwards
                for t in reversed(range(len(gaes) - 1)):
                    gaes[t] = gaes[t] + self.gamma * self.GAE_discount * gaes[t + 1]
                
                gaes_list.append(gaes)
            
            # Stack back to [T, n_veh]
            return np.stack(gaes_list, axis=1)
        else:
            # 1D case (MLP mode: [T])
            deltas = rewards + self.gamma * v_preds_next - v_preds
            gaes = deltas.copy()
            for t in reversed(range(len(gaes) - 1)):
                gaes[t] = gaes[t] + self.gamma * self.GAE_discount * gaes[t + 1]
            return gaes

    def get_gaes(self, rewards, v_preds, v_preds_next):
        """
        GAE
        :param rewards: r(t) - can be [T, n_veh] or [T]
        :param v_preds: v(st) - can be [T, n_veh] or [T]
        :param v_preds_next: v(st+1) - can be [T, n_veh] or [T]
        :return: gaes - same shape as input
        """
        # Convert to numpy arrays if needed
        rewards = np.array(rewards)
        v_preds = np.array(v_preds)
        v_preds_next = np.array(v_preds_next)
        
        # Handle 2D case (GAT mode: [T, n_veh])
        if len(rewards.shape) == 2:
            # For each agent, compute GAE separately
            n_veh = rewards.shape[1]
            gaes_list = []
            for agent_idx in range(n_veh):
                r_agent = rewards[:, agent_idx]
                v_agent = v_preds[:, agent_idx]
                v_next_agent = v_preds_next[:, agent_idx]
                
                # Compute deltas for this agent
                deltas = r_agent + self.gamma * v_next_agent - v_agent
                gaes = deltas.copy()
                
                # Compute GAE backwards
                for t in reversed(range(len(gaes) - 1)):
                    gaes[t] = gaes[t] + self.gamma * self.GAE_discount * gaes[t + 1]
                
                gaes_list.append(gaes)
            
            # Stack back to [T, n_veh]
            return np.stack(gaes_list, axis=1)
        else:
            # 1D case (MLP mode: [T])
            deltas = rewards + self.gamma * v_preds_next - v_preds
            gaes = deltas.copy()
            for t in reversed(range(len(gaes) - 1)):
                gaes[t] = gaes[t] + self.gamma * self.GAE_discount * gaes[t + 1]
            return gaes

    def averaging_model(self, success_rate, aggregation_weight=1.0, layer_wise=False, external_weights=None):
        """
        è”é‚¦å­¦ä¹ æ¨¡å‹èšåˆ
        Args:
            success_rate: å„UEçš„æˆåŠŸç‡ï¼Œç”¨äºè®¡ç®—èšåˆæƒé‡ï¼ˆå½“external_weightsä¸ºNoneæ—¶ä½¿ç”¨ï¼‰
            aggregation_weight: èšåˆæƒé‡ï¼ˆ0.0-1.0ï¼‰
                - 1.0: ç¡¬æ›¿æ¢ï¼ˆå®Œå…¨ä½¿ç”¨èšåˆå‚æ•°ï¼ŒåŸæœ‰é€»è¾‘ï¼‰
                - 0.7: è½¯èšåˆï¼ˆ70%èšåˆå‚æ•° + 30%æœ¬åœ°å‚æ•°ï¼‰
                - 0.0: ä¸èšåˆï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
            layer_wise: åˆ†å±‚è”é‚¦èšåˆå¼€å…³ï¼ˆé»˜è®¤Falseï¼‰
                - True: åªèšåˆç‰¹å¾æå–å±‚(w_1,w_2,b_1,b_2)ï¼Œä¿ç•™å†³ç­–å±‚ä¸ªæ€§åŒ–
                - False: èšåˆæ‰€æœ‰ç½‘ç»œå‚æ•°ï¼ˆæ ‡å‡†è”é‚¦å­¦ä¹ ï¼‰
            external_weights: å¤–éƒ¨æä¾›çš„èšåˆæƒé‡ï¼ˆå¦‚è¯­ä¹‰æ„ŸçŸ¥æƒé‡ï¼‰
                - å¦‚æœæä¾›ï¼Œå°†ä¼˜å…ˆä½¿ç”¨æ­¤æƒé‡è€Œésuccess_rateè®¡ç®—çš„æƒé‡
                - åº”ä¸ºå½’ä¸€åŒ–çš„numpyæ•°ç»„ï¼Œé•¿åº¦ä¸ºn_veh
        """
        # ä»…ä¿ç•™ MLP æ¨¡å¼ä¸‹çš„æ‰‹å·¥å‚æ•°å¹³å‡ï¼ˆè”é‚¦å­¦ä¹ ï¼‰
        # æ³¨æ„ï¼šå·²æ›´æ–°ä¸ºBetaåˆ†å¸ƒå‚æ•°
        # æ”¹è¿›ï¼šä½¿ç”¨åŸºäºsuccess_rateçš„åŠ æƒèšåˆï¼Œè€Œä¸æ˜¯ç®€å•å¹³å‡
        # æ”¹è¿›ï¼šæ”¯æŒè½¯èšåˆï¼ˆéƒ¨åˆ†æ›¿æ¢ï¼‰ï¼Œä¿ç•™éƒ¨åˆ†æœ¬åœ°å‚æ•°
        sigma = 1e-8
        
        # ç¡®ä¿aggregation_weightåœ¨æœ‰æ•ˆèŒƒå›´å†…
        aggregation_weight = np.clip(aggregation_weight, 0.0, 1.0)
        
        # æ‰“å°åˆ†å±‚èšåˆçŠ¶æ€
        if layer_wise:
            print("ğŸ”„ åˆ†å±‚è”é‚¦èšåˆ: åªèšåˆç‰¹å¾æå–å±‚(w_1,w_2,b_1,b_2)ï¼Œä¿ç•™å†³ç­–å±‚ä¸ªæ€§åŒ–")
        else:
            print("ğŸ”„ æ ‡å‡†è”é‚¦èšåˆ: èšåˆæ‰€æœ‰ç½‘ç»œå‚æ•°")
        
        # å¤„ç†æƒé‡ï¼šä¼˜å…ˆä½¿ç”¨external_weightsï¼Œå¦åˆ™ä½¿ç”¨success_rate
        if external_weights is not None:
            # ä½¿ç”¨å¤–éƒ¨æä¾›çš„æƒé‡ï¼ˆå¦‚è¯­ä¹‰æ„ŸçŸ¥æƒé‡ï¼‰
            weights = np.array(external_weights)
            # ç¡®ä¿æƒé‡å½’ä¸€åŒ–
            weights = weights / (weights.sum() + 1e-8)
            print(f"ğŸ”„ ä½¿ç”¨è¯­ä¹‰æ„ŸçŸ¥æƒé‡: {np.round(weights, 3)}")
        elif success_rate is not None and len(success_rate) > 0:
            # å°†success_rateå½’ä¸€åŒ–ä¸ºæƒé‡ï¼ˆé¿å…é™¤é›¶ï¼‰
            success_rate = np.array(success_rate)
            success_rate = np.clip(success_rate, 0.0, 1.0)  # ç¡®ä¿åœ¨[0,1]èŒƒå›´å†…
            # æ·»åŠ å°çš„epsiloné¿å…å…¨é›¶æƒ…å†µ
            weights = success_rate + 1e-6
            weights = weights / weights.sum()  # å½’ä¸€åŒ–
            print(f"ğŸ”„ ä½¿ç”¨æˆåŠŸç‡æƒé‡: {np.round(weights, 3)}")
        else:
            # å¦‚æœæ²¡æœ‰æƒé‡ä¿¡æ¯ï¼Œä½¿ç”¨å‡åŒ€æƒé‡
            weights = np.ones(self.n_veh) / self.n_veh
            print(f"ğŸ”„ ä½¿ç”¨å‡åŒ€æƒé‡: {np.round(weights, 3)}")
        
        # ç‰¹å¾æå–å±‚ (Encoder) - å§‹ç»ˆèšåˆ
        # ä¿®å¤ï¼šç´¯åŠ å™¨å¿…é¡»ä»é›¶å¼€å§‹ï¼Œä¸èƒ½ä»éšæœºæ•°å¼€å§‹ï¼
        w_1_mean = np.zeros([self.s_dim, n_hidden_1])
        w_2_mean = np.zeros([n_hidden_1, n_hidden_2])
        b_1_mean = np.zeros([n_hidden_1])
        b_2_mean = np.zeros([n_hidden_2])
        
        # å†³ç­–å±‚ (Task-specific Heads) - åªæœ‰åœ¨éåˆ†å±‚æ¨¡å¼ä¸‹æ‰èšåˆ
        if not layer_wise:
            w_3_mean = np.zeros([n_hidden_2, n_hidden_3])
            # Power Betaåˆ†å¸ƒå‚æ•°
            w_power_alpha_mean = np.zeros([n_hidden_2, 1])
            w_power_beta_mean = np.zeros([n_hidden_2, 1])
            w_RB_mean = np.zeros([n_hidden_2, self.n_RB])
            # Rho Betaåˆ†å¸ƒå‚æ•°
            w_rho_alpha_mean = np.zeros([n_hidden_2, 1])
            w_rho_beta_mean = np.zeros([n_hidden_2, 1])
            w_v_mean = np.zeros([n_hidden_3, 1])
            
            b_3_mean = np.zeros([n_hidden_3])
            # Power Betaåˆ†å¸ƒbias
            b_power_alpha_mean = np.zeros([1])
            b_power_beta_mean = np.zeros([1])
            b_RB_mean = np.zeros([self.n_RB])
            # Rho Betaåˆ†å¸ƒbias
            b_rho_alpha_mean = np.zeros([1])
            b_rho_beta_mean = np.zeros([1])
            b_v_mean = np.zeros([1])

        # ä½¿ç”¨åŠ æƒèšåˆï¼ˆåŸºäºæƒé‡ï¼‰
        # éªŒè¯ï¼šè®°å½•èšåˆå‰çš„å‚æ•°èŒƒå›´ï¼Œç”¨äºè°ƒè¯•
        param_ranges_before = {}
        for i in range(self.n_veh):
            w_1_sample = self.sesses[i].run(self.w_1)
            param_ranges_before[i] = {
                'w_1_min': np.min(w_1_sample),
                'w_1_max': np.max(w_1_sample),
                'w_1_mean': np.mean(w_1_sample)
            }
        
        for i in range(self.n_veh):
            weight = weights[i]
            # ç‰¹å¾æå–å±‚ (Encoder) - å§‹ç»ˆèšåˆ
            w_1_mean += self.sesses[i].run(self.w_1) * weight
            w_2_mean += self.sesses[i].run(self.w_2) * weight
            b_1_mean += self.sesses[i].run(self.b_1) * weight
            b_2_mean += self.sesses[i].run(self.b_2) * weight
            
            # å†³ç­–å±‚ (Task-specific Heads) - åªæœ‰åœ¨éåˆ†å±‚æ¨¡å¼ä¸‹æ‰èšåˆ
            if not layer_wise:
                w_3_mean += self.sesses[i].run(self.w_3) * weight
                w_power_alpha_mean += self.sesses[i].run(self.w_power_alpha) * weight
                w_power_beta_mean += self.sesses[i].run(self.w_power_beta) * weight
                w_RB_mean += self.sesses[i].run(self.w_RB) * weight
                w_rho_alpha_mean += self.sesses[i].run(self.w_rho_alpha) * weight
                w_rho_beta_mean += self.sesses[i].run(self.w_rho_beta) * weight
                w_v_mean += self.sesses[i].run(self.w_v) * weight

                b_3_mean += self.sesses[i].run(self.b_3) * weight
                b_power_alpha_mean += self.sesses[i].run(self.b_power_alpha) * weight
                b_power_beta_mean += self.sesses[i].run(self.b_power_beta) * weight
                b_RB_mean += self.sesses[i].run(self.b_RB) * weight
                b_rho_alpha_mean += self.sesses[i].run(self.b_rho_alpha) * weight
                b_rho_beta_mean += self.sesses[i].run(self.b_rho_beta) * weight
                b_v_mean += self.sesses[i].run(self.b_v) * weight
        
        # éªŒè¯ï¼šæ£€æŸ¥èšåˆåçš„å‚æ•°èŒƒå›´
        print(f"ğŸ“Š èšåˆéªŒè¯: w_1_meanèŒƒå›´=[{np.min(w_1_mean):.4f}, {np.max(w_1_mean):.4f}], å‡å€¼={np.mean(w_1_mean):.4f}")
        if layer_wise:
            print(f"ğŸ“Š åˆ†å±‚èšåˆ: åªæ›´æ–°ç‰¹å¾å±‚(w_1,w_2,b_1,b_2)ï¼Œå†³ç­–å±‚ä¿æŒä¸å˜")
        else:
            print(f"ğŸ“Š æ ‡å‡†èšåˆ: æ›´æ–°æ‰€æœ‰å±‚")

        # è½¯èšåˆï¼šæ··åˆèšåˆå‚æ•°å’Œæœ¬åœ°å‚æ•°
        for i in range(self.n_veh):
            if aggregation_weight < 1.0:
                # è½¯èšåˆï¼šä¿ç•™éƒ¨åˆ†æœ¬åœ°å‚æ•°
                # è·å–å½“å‰æœ¬åœ°å‚æ•° - ç‰¹å¾æå–å±‚
                old_w_1 = self.sesses[i].run(self.w_1)
                old_w_2 = self.sesses[i].run(self.w_2)
                old_b_1 = self.sesses[i].run(self.b_1)
                old_b_2 = self.sesses[i].run(self.b_2)
                
                # è·å–å†³ç­–å±‚å‚æ•°ï¼ˆä»…éåˆ†å±‚æ¨¡å¼éœ€è¦ï¼‰
                if not layer_wise:
                    old_w_3 = self.sesses[i].run(self.w_3)
                    old_w_power_alpha = self.sesses[i].run(self.w_power_alpha)
                    old_w_power_beta = self.sesses[i].run(self.w_power_beta)
                    old_w_RB = self.sesses[i].run(self.w_RB)
                    old_w_rho_alpha = self.sesses[i].run(self.w_rho_alpha)
                    old_w_rho_beta = self.sesses[i].run(self.w_rho_beta)
                    old_w_v = self.sesses[i].run(self.w_v)
                    
                    old_b_3 = self.sesses[i].run(self.b_3)
                    old_b_power_alpha = self.sesses[i].run(self.b_power_alpha)
                    old_b_power_beta = self.sesses[i].run(self.b_power_beta)
                    old_b_RB = self.sesses[i].run(self.b_RB)
                    old_b_rho_alpha = self.sesses[i].run(self.b_rho_alpha)
                    old_b_rho_beta = self.sesses[i].run(self.b_rho_beta)
                    old_b_v = self.sesses[i].run(self.b_v)
                
                # è½¯èšåˆï¼šæ··åˆæ–°æ—§å‚æ•° - ç‰¹å¾æå–å±‚
                # new_param = aggregation_weight * aggregated_param + (1 - aggregation_weight) * local_param
                new_w_1 = aggregation_weight * w_1_mean + (1 - aggregation_weight) * old_w_1
                new_w_2 = aggregation_weight * w_2_mean + (1 - aggregation_weight) * old_w_2
                new_b_1 = aggregation_weight * b_1_mean + (1 - aggregation_weight) * old_b_1
                new_b_2 = aggregation_weight * b_2_mean + (1 - aggregation_weight) * old_b_2
                
                # éªŒè¯ï¼šæ£€æŸ¥è½¯èšåˆæ˜¯å¦ç”Ÿæ•ˆï¼ˆç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“ï¼‰
                if i == 0:
                    change_w_1 = np.mean(np.abs(new_w_1 - old_w_1))
                    change_agg = np.mean(np.abs(w_1_mean - old_w_1))
                    print(f"ğŸ’¡ è½¯èšåˆéªŒè¯ (Agent 0): w_1å˜åŒ–={change_w_1:.6f}, ç¡¬æ›¿æ¢å˜åŒ–={change_agg:.6f}, è½¯èšåˆæ¯”ä¾‹={aggregation_weight:.2f}")
                
                self.sesses[i].run(self.w_1.assign(new_w_1))
                self.sesses[i].run(self.w_2.assign(new_w_2))
                self.sesses[i].run(self.b_1.assign(new_b_1))
                self.sesses[i].run(self.b_2.assign(new_b_2))
                
                # å†³ç­–å±‚ï¼šåªæœ‰åœ¨éåˆ†å±‚æ¨¡å¼ä¸‹æ‰è¿›è¡Œèšåˆæ›´æ–°
                if not layer_wise:
                    self.sesses[i].run(self.w_3.assign(aggregation_weight * w_3_mean + (1 - aggregation_weight) * old_w_3))
                    self.sesses[i].run(self.w_power_alpha.assign(aggregation_weight * w_power_alpha_mean + (1 - aggregation_weight) * old_w_power_alpha))
                    self.sesses[i].run(self.w_power_beta.assign(aggregation_weight * w_power_beta_mean + (1 - aggregation_weight) * old_w_power_beta))
                    self.sesses[i].run(self.w_RB.assign(aggregation_weight * w_RB_mean + (1 - aggregation_weight) * old_w_RB))
                    self.sesses[i].run(self.w_rho_alpha.assign(aggregation_weight * w_rho_alpha_mean + (1 - aggregation_weight) * old_w_rho_alpha))
                    self.sesses[i].run(self.w_rho_beta.assign(aggregation_weight * w_rho_beta_mean + (1 - aggregation_weight) * old_w_rho_beta))
                    self.sesses[i].run(self.w_v.assign(aggregation_weight * w_v_mean + (1 - aggregation_weight) * old_w_v))
                    
                    self.sesses[i].run(self.b_3.assign(aggregation_weight * b_3_mean + (1 - aggregation_weight) * old_b_3))
                    self.sesses[i].run(self.b_power_alpha.assign(aggregation_weight * b_power_alpha_mean + (1 - aggregation_weight) * old_b_power_alpha))
                    self.sesses[i].run(self.b_power_beta.assign(aggregation_weight * b_power_beta_mean + (1 - aggregation_weight) * old_b_power_beta))
                    self.sesses[i].run(self.b_RB.assign(aggregation_weight * b_RB_mean + (1 - aggregation_weight) * old_b_RB))
                    self.sesses[i].run(self.b_rho_alpha.assign(aggregation_weight * b_rho_alpha_mean + (1 - aggregation_weight) * old_b_rho_alpha))
                    self.sesses[i].run(self.b_rho_beta.assign(aggregation_weight * b_rho_beta_mean + (1 - aggregation_weight) * old_b_rho_beta))
                    self.sesses[i].run(self.b_v.assign(aggregation_weight * b_v_mean + (1 - aggregation_weight) * old_b_v))
            else:
                # ç¡¬æ›¿æ¢ï¼ˆåŸæœ‰é€»è¾‘ï¼‰ï¼šå®Œå…¨ä½¿ç”¨èšåˆå‚æ•°
                # ç‰¹å¾æå–å±‚ - å§‹ç»ˆæ›´æ–°
                self.sesses[i].run(self.w_1.assign(w_1_mean))
                self.sesses[i].run(self.w_2.assign(w_2_mean))
                self.sesses[i].run(self.b_1.assign(b_1_mean))
                self.sesses[i].run(self.b_2.assign(b_2_mean))
                
                # å†³ç­–å±‚ - åªæœ‰åœ¨éåˆ†å±‚æ¨¡å¼ä¸‹æ‰æ›´æ–°
                if not layer_wise:
                    self.sesses[i].run(self.w_3.assign(w_3_mean))
                    self.sesses[i].run(self.w_power_alpha.assign(w_power_alpha_mean))
                    self.sesses[i].run(self.w_power_beta.assign(w_power_beta_mean))
                    self.sesses[i].run(self.w_RB.assign(w_RB_mean))
                    self.sesses[i].run(self.w_rho_alpha.assign(w_rho_alpha_mean))
                    self.sesses[i].run(self.w_rho_beta.assign(w_rho_beta_mean))
                    self.sesses[i].run(self.w_v.assign(w_v_mean))
                    
                    self.sesses[i].run(self.b_3.assign(b_3_mean))
                    self.sesses[i].run(self.b_power_alpha.assign(b_power_alpha_mean))
                    self.sesses[i].run(self.b_power_beta.assign(b_power_beta_mean))
                    self.sesses[i].run(self.b_RB.assign(b_RB_mean))
                    self.sesses[i].run(self.b_rho_alpha.assign(b_rho_alpha_mean))
                    self.sesses[i].run(self.b_rho_beta.assign(b_rho_beta_mean))
                    self.sesses[i].run(self.b_v.assign(b_v_mean))
